{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Open-Source Multi-Cloud Asset to build SaaS (Documentation) \u00b6 Development and automated deployment of SaaS for multiple tenants, using Red Hat OpenShift/Kubernetes and DevSecOps Documentation Overview \u00b6 Introduction Development of Microservices Quarkus Backend Service Code Quarkus Backend Service Container Vue.js Frontend Service Code Vue.js Frontend Service Container Externalization of Variables in Backend Microservices Externalization of Variables in Frontend Microservices Local Development of Services Authentication Flow (AppID, backend, frontend) Creation of managed IBM Cloud Services Database Programmatic Creation of Postgres Programmatic Configuration of Postgres including Schema Authentication Programmatic Creation of AppID Programmatic Configuration of AppID Serverless via IBM Code Engine Architecture Getting started via Scripts Create the tenants with the related IBM Cloud services Verify the tenants with the related IBM Cloud services instances CI/CD Onboarding Observability (logging, monitoring, vulnerabilities) Billing Clean Up Kubernetes via IBM Kubernetes Service and IBM OpenShift Architecture Initial Setup via Scripts CI/CD DevSecOps Overview CI CI pull request CI pipeline CD CD pull request CD pipeline Security and Compliance Setup of the Toolchains CI Toolchains CD Toolchains Onboarding Observability (logging, monitoring, vulnerabilities) Billing","title":"Open-Source Multi-Cloud Asset to build SaaS (Documentation)"},{"location":"#open-source-multi-cloud-asset-to-build-saas-documentation","text":"Development and automated deployment of SaaS for multiple tenants, using Red Hat OpenShift/Kubernetes and DevSecOps","title":"Open-Source Multi-Cloud Asset to build SaaS (Documentation)"},{"location":"#documentation-overview","text":"Introduction Development of Microservices Quarkus Backend Service Code Quarkus Backend Service Container Vue.js Frontend Service Code Vue.js Frontend Service Container Externalization of Variables in Backend Microservices Externalization of Variables in Frontend Microservices Local Development of Services Authentication Flow (AppID, backend, frontend) Creation of managed IBM Cloud Services Database Programmatic Creation of Postgres Programmatic Configuration of Postgres including Schema Authentication Programmatic Creation of AppID Programmatic Configuration of AppID Serverless via IBM Code Engine Architecture Getting started via Scripts Create the tenants with the related IBM Cloud services Verify the tenants with the related IBM Cloud services instances CI/CD Onboarding Observability (logging, monitoring, vulnerabilities) Billing Clean Up Kubernetes via IBM Kubernetes Service and IBM OpenShift Architecture Initial Setup via Scripts CI/CD DevSecOps Overview CI CI pull request CI pipeline CD CD pull request CD pipeline Security and Compliance Setup of the Toolchains CI Toolchains CD Toolchains Onboarding Observability (logging, monitoring, vulnerabilities) Billing","title":"Documentation Overview"},{"location":"README_introduction/","text":"Introduction \u00b6 Open-Source Multi-Cloud Asset to build SaaS Development and automated deployment of SaaS for multiple tenants, using Red Hat OpenShift/Kubernetes and DevSecOps A key benefit of the cloud is the ability to deploy software for multiple consumers without having to install it redundantly on-premises. When software is provided as a managed service (SaaS), costs can be reduced for the deployments and the operations of applications. Additionally SaaS can be scaled and new consumers can be added easily. In order to leverage these advantages, applications need to be designed, so that they can support multiple tenants. Often tenants are not single users, but clients of SaaS providers with their own corporate authentication mechanisms. When running SaaS for multiple tenants, it's often required to keep the workloads isolated from each other for security reasons. For example, typically separate databases are used for tenants. At the same time common deployment and operation models are required, so that new SaaS versions can be deployed to different tenants in an unique and efficient way. This project aims to support IBM partners to build SaaS for different platforms including Kubernetes, OpenShift, Serverless, Satellite, AWS and Azure. The used sample application, which contains two containers, is the same one for all platforms. The CI/CD mechanisms slightly differentiate between the platforms. Platform Options \u00b6 The following diagram shows the different platform options. At this point the repo contains the IBM Cloud platforms. More options are planned to be added. For example with Satellite the SaaS application can be deployed on-premises to client data centers, but managed centrally. Additionally the same SaaS application can be deployed on other managed OpenShift services like AWS ROSA and Azure ARO. Serverless on IBM Cloud \u00b6 The easiest way to get started is to use serverless. The repo describes how to use IBM Code Engine to run the application logic, IBM App ID for authentication, IBM Postgres for persistence and IBM Toolchain for CI/CD. Scripts are provided to make the setup as easy as possible. Managed Kubernetes and OpenShift on IBM Cloud \u00b6 For more advanced cloud-native applications Kubernetes and OpenShift can be used. Compute isolation can be done either by sharing clusters and using Kubernetes namespaces/OpenShift projects or by having separate clusters for tenants. For authentication the managed services App ID and Postgres can be used, but they can also be replaced by other managed services or services running within the clusters. For CI/CD the IBM DevSecOps reference architecture based on IBM Toolchain is used which is also the internal IBM standard and which guarantees compliance for regulated industries. Sample Application \u00b6 The project comes with a simple e-commerce example application. A SaaS provider might have one client selling books, another one selling shoes. Repositories \u00b6 This repo is the 'parent repo' including documentation and global configuration. The other four repos contain the implementation of the microservices and the serverless pipelines. multi-tenancy - this repo (parent repo) Overview documentation Global and tenant specific application configuration CD pipeline Scripts to deploy cloud services/infrastructure multi-tenancy-backend - backend microservice Code CI pipeline multi-tenancy-frontend - frontend microservice Code CI pipeline multi-tenancy-serverless-ci-cd - CI and CD pipelines for serverless Getting Started \u00b6 The easiest way to get started is to set up the sample application for two tenants on the IBM Cloud using serverless technology. The following diagram describes the serverless architecture of the simple e-commerce application which has two images (backend and frontend). Isolated Compute: One frontend container per tenant One backend container per tenant One App ID instance per tenant One Postgres instance (with one database) per tenant Shared CI/CD: One code base for frontend and backend services One image for frontend service One image for backend service One toolchain for all tenants (with four pipelines) Used IBM Services: IBM Code Engine IBM Container Registry IBM App ID IBM Postgres IBM Toolchain Used Technologies: Quarkus Vue.js and nginx Bash scripts Initial Deployment Scripts \u00b6 Scripts and provided to set up all services and the application automatically. Follow this step by step guide to set up everything using local bash scripts. Deployments of Updates via CI/CD \u00b6 Additionally pipelines are provided to re-deploy the backend and frontend services when their implementations have changed. Follow this step by step guide to set up the pipelines.","title":"Introduction"},{"location":"README_introduction/#introduction","text":"Open-Source Multi-Cloud Asset to build SaaS Development and automated deployment of SaaS for multiple tenants, using Red Hat OpenShift/Kubernetes and DevSecOps A key benefit of the cloud is the ability to deploy software for multiple consumers without having to install it redundantly on-premises. When software is provided as a managed service (SaaS), costs can be reduced for the deployments and the operations of applications. Additionally SaaS can be scaled and new consumers can be added easily. In order to leverage these advantages, applications need to be designed, so that they can support multiple tenants. Often tenants are not single users, but clients of SaaS providers with their own corporate authentication mechanisms. When running SaaS for multiple tenants, it's often required to keep the workloads isolated from each other for security reasons. For example, typically separate databases are used for tenants. At the same time common deployment and operation models are required, so that new SaaS versions can be deployed to different tenants in an unique and efficient way. This project aims to support IBM partners to build SaaS for different platforms including Kubernetes, OpenShift, Serverless, Satellite, AWS and Azure. The used sample application, which contains two containers, is the same one for all platforms. The CI/CD mechanisms slightly differentiate between the platforms.","title":"Introduction"},{"location":"README_introduction/#platform-options","text":"The following diagram shows the different platform options. At this point the repo contains the IBM Cloud platforms. More options are planned to be added. For example with Satellite the SaaS application can be deployed on-premises to client data centers, but managed centrally. Additionally the same SaaS application can be deployed on other managed OpenShift services like AWS ROSA and Azure ARO.","title":"Platform Options"},{"location":"README_introduction/#serverless-on-ibm-cloud","text":"The easiest way to get started is to use serverless. The repo describes how to use IBM Code Engine to run the application logic, IBM App ID for authentication, IBM Postgres for persistence and IBM Toolchain for CI/CD. Scripts are provided to make the setup as easy as possible.","title":"Serverless on IBM Cloud"},{"location":"README_introduction/#managed-kubernetes-and-openshift-on-ibm-cloud","text":"For more advanced cloud-native applications Kubernetes and OpenShift can be used. Compute isolation can be done either by sharing clusters and using Kubernetes namespaces/OpenShift projects or by having separate clusters for tenants. For authentication the managed services App ID and Postgres can be used, but they can also be replaced by other managed services or services running within the clusters. For CI/CD the IBM DevSecOps reference architecture based on IBM Toolchain is used which is also the internal IBM standard and which guarantees compliance for regulated industries.","title":"Managed Kubernetes and OpenShift on IBM Cloud"},{"location":"README_introduction/#sample-application","text":"The project comes with a simple e-commerce example application. A SaaS provider might have one client selling books, another one selling shoes.","title":"Sample Application"},{"location":"README_introduction/#repositories","text":"This repo is the 'parent repo' including documentation and global configuration. The other four repos contain the implementation of the microservices and the serverless pipelines. multi-tenancy - this repo (parent repo) Overview documentation Global and tenant specific application configuration CD pipeline Scripts to deploy cloud services/infrastructure multi-tenancy-backend - backend microservice Code CI pipeline multi-tenancy-frontend - frontend microservice Code CI pipeline multi-tenancy-serverless-ci-cd - CI and CD pipelines for serverless","title":"Repositories"},{"location":"README_introduction/#getting-started","text":"The easiest way to get started is to set up the sample application for two tenants on the IBM Cloud using serverless technology. The following diagram describes the serverless architecture of the simple e-commerce application which has two images (backend and frontend). Isolated Compute: One frontend container per tenant One backend container per tenant One App ID instance per tenant One Postgres instance (with one database) per tenant Shared CI/CD: One code base for frontend and backend services One image for frontend service One image for backend service One toolchain for all tenants (with four pipelines) Used IBM Services: IBM Code Engine IBM Container Registry IBM App ID IBM Postgres IBM Toolchain Used Technologies: Quarkus Vue.js and nginx Bash scripts","title":"Getting Started"},{"location":"README_introduction/#initial-deployment-scripts","text":"Scripts and provided to set up all services and the application automatically. Follow this step by step guide to set up everything using local bash scripts.","title":"Initial Deployment Scripts"},{"location":"README_introduction/#deployments-of-updates-via-cicd","text":"Additionally pipelines are provided to re-deploy the backend and frontend services when their implementations have changed. Follow this step by step guide to set up the pipelines.","title":"Deployments of Updates via CI/CD"},{"location":"placeholder/","text":"Welcome to the placeholder \u00b6 This part of the documentation needs to be defined.","title":"Architecture"},{"location":"placeholder/#welcome-to-the-placeholder","text":"This part of the documentation needs to be defined.","title":"Welcome to the placeholder"},{"location":"automation/terraform/1-Provisionning-The-Infrastructure/","text":"Provisionning The Infrastructure \u00b6 This document describes how to provision the back-end infrastructure for your project. The infrastructure consists on; A \"Virtual Private Cloud\" (VPC) on IBM Cloud. An OpenShift cluster inside the VPC with all the requirements. A Database for PostgreSQL managed service. An instance of AppID service. Setting up the infrastructure \u00b6 The steps to follow to provision the infrastrure are, Step 1: Open a terminal window in the current directory where the code reside. \u00b6 Log into IBM Cloud ibmcloud login --sso Step 2: Make a copy of \"credentials.template\" file and name it as \"credential.properties\". \u00b6 cp credentials.template credential.properties Step 3: Edit the \"credentials.properties\" file and put your IBM Cloud Api Key. \u00b6 # Add the values for the Credentials to access the IBM Cloud # Instructions to access this information can be found in the README.MD # This is a template file and the ./launch.sh script looks for a file based on this template named credentials.properties ibmcloud.api.key = \"your-api-key\" Step 4: Save and exit the file. \u00b6 Open a terminal window and run the following command: ./apply-all.sh `","title":"Provisionning The Infrastructure"},{"location":"automation/terraform/1-Provisionning-The-Infrastructure/#provisionning-the-infrastructure","text":"This document describes how to provision the back-end infrastructure for your project. The infrastructure consists on; A \"Virtual Private Cloud\" (VPC) on IBM Cloud. An OpenShift cluster inside the VPC with all the requirements. A Database for PostgreSQL managed service. An instance of AppID service.","title":"Provisionning The Infrastructure"},{"location":"automation/terraform/1-Provisionning-The-Infrastructure/#setting-up-the-infrastructure","text":"The steps to follow to provision the infrastrure are,","title":"Setting up the infrastructure"},{"location":"automation/terraform/1-Provisionning-The-Infrastructure/#step-1-open-a-terminal-window-in-the-current-directory-where-the-code-reside","text":"Log into IBM Cloud ibmcloud login --sso","title":"Step 1: Open a terminal window in the current directory where the code reside."},{"location":"automation/terraform/1-Provisionning-The-Infrastructure/#step-2-make-a-copy-of-credentialstemplate-file-and-name-it-as-credentialproperties","text":"cp credentials.template credential.properties","title":"Step 2: Make a copy of \"credentials.template\" file and name it as \"credential.properties\"."},{"location":"automation/terraform/1-Provisionning-The-Infrastructure/#step-3-edit-the-credentialsproperties-file-and-put-your-ibm-cloud-api-key","text":"# Add the values for the Credentials to access the IBM Cloud # Instructions to access this information can be found in the README.MD # This is a template file and the ./launch.sh script looks for a file based on this template named credentials.properties ibmcloud.api.key = \"your-api-key\"","title":"Step 3: Edit the \"credentials.properties\" file and put your IBM Cloud Api Key."},{"location":"automation/terraform/1-Provisionning-The-Infrastructure/#step-4-save-and-exit-the-file","text":"Open a terminal window and run the following command: ./apply-all.sh `","title":"Step 4: Save and exit the file."},{"location":"automation/terraform/2-Provisionning-The-Minimal-Infrastructure/","text":"Provisionning The Minimal Infrastructure \u00b6 This document describes how to provision the back-end infrastructure for your project. As a prerequisite, Terraform should be installed on the machine used for the following operations. The infrastructure consists on; A \"Virtual Private Cloud\" (VPC) on IBM Cloud. Either an OpenShift or an IKS cluster inside the VPC with all the requirements. A Database for PostgreSQL managed service. An instance of AppID service. Setting up the infrastructure - OpenShift Cluster \u00b6 The steps to follow to provision the infrastrure are, Step 1: Clone the following Github repo; \u00b6 git clone https://github.com/cloud-native-toolkit/iascable ` Step 2: Go to the cloned folder; \u00b6 cd iascable Step 3: Install the required modules and packages \u00b6 npm install and npm run build Step 4: Create the examples/baseline-openshift.yaml Yaml file; \u00b6 apiVersion : cloud.ibm.com/v1alpha1 kind : BillOfMaterial metadata : name : baseline-openshift spec : modules : - name : ibm-resource-group - name : ibm-vpc - name : ibm-vpc-gateways - name : ibm-vpc-subnets alias : cluster-subnets variables : - name : subnet_count value : 1 - name : subnet_label value : cluster - name : ibm-ocp-vpc dependencies : - name : subnets ref : cluster-subnets Step 5: Log into IBM Cloud \u00b6 ibmcloud login --sso Step 6: Run the following command from the original folder; \u00b6 ./iascable build -i ./examples/baseline-openshift.yaml Step 7: Edit the \"/iascable/output/baseline-openshift.auto.tfvars\" and enter values for the following parameters; \u00b6 resource_group_name ibmcloud_api_key region name_prefix namespace_name ## resource_group_name: The name of the resource group resource_group_name = \"your-resource-group-name\" ## region: The IBM Cloud region where the cluster will be/has been installed. region = \"eu-de\" (or other IBM Cloud regions) ## ibmcloud_api_key: The IBM Cloud api token ibmcloud_api_key = \"your-ibm-cloud-api-key\" ## namespace_name: The namespace that should be created namespace_name = \"your-namesspace-name\" \u200bSave and quit the file. Step 8: Edit the \"credential.properties\" file and complete it as the following; \u00b6 # Add the values for the Credentials to access the IBM Cloud # Instructions to access this information can be found in the README.MD classic.username = \"your-ibm-cloud-account-ID\" classic.api.key = \"your-ibm-cloud-classic-api-key\" ibmcloud.api.key = \"your-ibm-cloud-api-key\" # Authentication to OCP can either be performed with username/password or token # If token is provided it will take precedence login.user = \"\" login.password = \"\" login.token = \"\" server.url = \"\" Save and quit the file. Step 9: Open a terminal window and run the following command: \u00b6 cd output/baseline-openshift/terraform terraform init terraform plan terraform apply Setting up the infrastructure - IKS (IBM Kubernetes Services) Cluster \u00b6 The steps to follow to provision the infrastrure are, Clone the following Github repo; git clone https://github.com/cloud-native-toolkit/iascable ` Go to the cloned folder; cd iascable Install the required modules and packages npm install and npm run build Create the examples/baseline-iks.yaml Yaml file; apiVersion : cloud.ibm.com/v1alpha1 kind : BillOfMaterial metadata : name : baseline-openshift spec : modules : - name : ibm-resource-group - name : ibm-vpc - name : ibm-vpc-gateways - name : ibm-vpc-subnets alias : cluster-subnets variables : - name : subnet_count value : 1 - name : subnet_label value : cluster - name : ibm-ocp-vpc dependencies : - name : subnets ref : cluster-subnets Log into IBM Cloud ibmcloud login --sso Run the following command from the original folder; ./iascable build -i ./examples/baseline-iks.yaml Edit the \"/iascable/output/baseline-openshift.auto.tfvars\" and enter values for the following parameters; resource_group_name ibmcloud_api_key region name_prefix ## resource_group_name: The name of the resource group resource_group_name = \"your-resource-group-name\" ## region: The IBM Cloud region where the cluster will be/has been installed. region = \"eu-de\" (or other IBM Cloud regions) ## ibmcloud_api_key: The IBM Cloud api token ibmcloud_api_key = \"your-ibm-cloud-api-key\" \u200b Save and quit the file. Edit the \"credential.properties\" file and complete it as the following; # Add the values for the Credentials to access the IBM Cloud # Instructions to access this information can be found in the README.MD classic.username = \"your-ibm-cloud-account-ID\" classic.api.key = \"your-ibm-cloud-classic-api-key\" ibmcloud.api.key = \"your-ibm-cloud-api-key\" # Authentication to OCP can either be performed with username/password or token # If token is provided it will take precedence login.user = \"\" login.password = \"\" login.token = \"\" server.url = \"\" Save and quit the file. Open a terminal window and run the following command: cd output/baseline-openshift/terraform terraform init terraform plan terraform apply Changing some default variables \u00b6 Number of Worker Pools and Worker Nodes \u00b6 In both OpenShift and IKS, the defaut configuration provides a cluster with 3 worker pools and 3 worker nodes per pool. If a smaller cluster is needed, for OpenShift the default value can be changed in sub folder \"../baseline-openshift/terraform/variables.tf\" through the \"worker-count\" variable. variable \"worker_count\" { type = number description = \"The number of worker nodes that should be provisioned for classic infrastructure\" default = 3 } For IKS the default value can be changed in sub folder \"../baseline-iks/terraform/variables.tf\" through the \"worker-count\" same variable. Changing the default cluster flavor \u00b6 For both OCP/IKS, the default values for the cluster falvor are set in the same \"variables.tf\" as mentioned above. For a change of the cluster flavor, modify the \"cluster_flavor\" variable (https://cloud.ibm.com/docs/containers?topic=containers-clusters). variable \"cluster_flavor\" { type = string description = \"The machine type that will be provisioned for classic infrastructure\" default = \"bx2.4x16\" } To obtain the list of available flavors for IKS clusters proceed as shown; ibmcloud login ( or ibmcloud login --sso if not done already ) ibmcloud target -g <resource_group_name> ibmcloud ks locations Example output: \u276f ibmcloud ks locations VPC Infrastructure Zones Zone Metro Country Geography eu-gb-3 London ( lon ) United Kingdom ( uk ) Europe ( eu ) jp-tok-2 Tokyo ( tok ) Japan ( jp ) Asia Pacific ( ap ) eu-de-1 Frankfurt ( fra ) Germany ( de ) Europe ( eu ) us-east-3 Washington DC ( wdc ) United States ( us ) North America ( na ) ca-tor-3 Toronto ( tor ) Canada ( ca ) North America ( na ) br-sao-3 Sao Paulo ( sao ) Brazil ( br ) South America ( sa ) ca-tor-2 Toronto ( tor ) Canada ( ca ) North America ( na ) jp-osa-2 Osaka ( osa ) Japan ( jp ) Asia Pacific ( ap ) us-east-2 Washington DC ( wdc ) United States ( us ) North America ( na ) au-syd-3 Sydney ( syd ) Australia ( au ) Asia Pacific ( ap ) au-syd-2 Sydney ( syd ) Australia ( au ) Asia Pacific ( ap ) jp-osa-1 Osaka ( osa ) Japan ( jp ) Asia Pacific ( ap ) br-sao-2 Sao Paulo ( sao ) Brazil ( br ) South America ( sa ) eu-gb-1 London ( lon ) United Kingdom ( uk ) Europe ( eu ) eu-de-3 Frankfurt ( fra ) Germany ( de ) Europe ( eu ) jp-tok-3 Tokyo ( tok ) Japan ( jp ) Asia Pacific ( ap ) au-syd-1 Sydney ( syd ) Australia ( au ) Asia Pacific ( ap ) br-sao-1 Sao Paulo ( sao ) Brazil ( br ) South America ( sa ) jp-osa-3 Osaka ( osa ) Japan ( jp ) Asia Pacific ( ap ) us-south-2 Dallas ( dal ) United States ( us ) North America ( na ) jp-tok-1 Tokyo ( tok ) Japan ( jp ) Asia Pacific ( ap ) ca-tor-1 Toronto ( tor ) Canada ( ca ) North America ( na ) eu-gb-2 London ( lon ) United Kingdom ( uk ) Europe ( eu ) us-east-1 Washington DC ( wdc ) United States ( us ) North America ( na ) us-south-1 Dallas ( dal ) United States ( us ) North America ( na ) us-south-3 Dallas ( dal ) United States ( us ) North America ( na ) eu-de-2 Frankfurt ( fra ) Germany ( de ) Europe ( eu ) Classic Infrastructure Zones Zone Metro Country Geography mil01 Milan ( mil ) Italy ( it ) Europe ( eu ) osl01 Oslo ( osl ) Norway ( no ) Europe ( eu ) osa23 Osaka ( osa ) \u2020 Japan ( jp ) Asia Pacific ( ap ) lon06 London ( lon ) \u2020 United Kingdom ( uk ) Europe ( eu ) lon02 London ( lon ) \u2020 United Kingdom ( uk ) Europe ( eu ) che01 Chennai ( che ) India ( in ) Asia Pacific ( ap ) lon04 London ( lon ) \u2020 United Kingdom ( uk ) Europe ( eu ) seo01 Seoul ( seo ) Korea ( kr ) Asia Pacific ( ap ) dal12 Dallas ( dal ) \u2020 United States ( us ) North America ( na ) dal10 Dallas ( dal ) \u2020 United States ( us ) North America ( na ) wdc04 Washington DC ( wdc ) \u2020 United States ( us ) North America ( na ) osa21 Osaka ( osa ) \u2020 Japan ( jp ) Asia Pacific ( ap ) osa22 Osaka ( osa ) \u2020 Japan ( jp ) Asia Pacific ( ap ) sjc03 San Jose ( sjc ) United States ( us ) North America ( na ) mex01 Mexico City ( mex-cty ) Mexico ( mex ) North America ( na ) syd04 Sydney ( syd ) \u2020 Australia ( au ) Asia Pacific ( ap ) hkg02 Hong Kong ( hkg-mtr ) Hong Kong ( hkg ) Asia Pacific ( ap ) mon01 Montreal ( mon ) Canada ( ca ) North America ( na ) tok04 Tokyo ( tok ) \u2020 Japan ( jp ) Asia Pacific ( ap ) par01 Paris ( par ) France ( fr ) Europe ( eu ) syd01 Sydney ( syd ) \u2020 Australia ( au ) Asia Pacific ( ap ) wdc07 Washington DC ( wdc ) \u2020 United States ( us ) North America ( na ) ams03 Amsterdam ( ams ) Netherlands ( nl ) Europe ( eu ) fra04 Frankfurt ( fra ) \u2020 Germany ( de ) Europe ( eu ) tor01 Toronto ( tor ) Canada ( ca ) North America ( na ) fra05 Frankfurt ( fra ) \u2020 Germany ( de ) Europe ( eu ) sjc04 San Jose ( sjc ) United States ( us ) North America ( na ) tok02 Tokyo ( tok ) \u2020 Japan ( jp ) Asia Pacific ( ap ) hou02 Houston ( hou ) United States ( us ) North America ( na ) sao01 Sao Paulo ( sao ) Brazil ( br ) South America ( sa ) lon05 London ( lon ) \u2020 United Kingdom ( uk ) Europe ( eu ) tok05 Tokyo ( tok ) \u2020 Japan ( jp ) Asia Pacific ( ap ) wdc06 Washington DC ( wdc ) \u2020 United States ( us ) North America ( na ) syd05 Sydney ( syd ) \u2020 Australia ( au ) Asia Pacific ( ap ) dal13 Dallas ( dal ) \u2020 United States ( us ) North America ( na ) sng01 Singapore ( sng-mtr ) Singapore ( sng ) Asia Pacific ( ap ) fra02 Frankfurt ( fra ) \u2020 Germany ( de ) Europe ( eu ) \u2020 denotes the zone is in a multizone region. >>ibmcloud ks flavors --zone eu-de-1 For more information about these flavors, see 'https://ibm.biz/flavors' Name Cores Memory Network Speed OS Server Type Storage Secondary Storage Flavor Class Provider bx2.16x64 16 64GB 16Gbps UBUNTU_18_64 virtual 100GB 0B bx2 vpc-gen2 bx2.2x8\u2020 2 8GB 4Gbps UBUNTU_18_64 virtual 100GB 0B bx2 vpc-gen2 bx2.32x128 32 128GB 16Gbps UBUNTU_18_64 virtual 100GB 0B bx2 vpc-gen2 bx2.48x192 48 192GB 16Gbps UBUNTU_18_64 virtual 100GB 0B bx2 vpc-gen2 bx2.4x16 4 16GB 8Gbps UBUNTU_18_64 virtual 100GB 0B bx2 vpc-gen2 bx2.8x32 8 32GB 16Gbps UBUNTU_18_64 virtual 100GB 0B bx2 vpc-gen2 cx2.16x32 16 32GB 16Gbps UBUNTU_18_64 virtual 100GB 0B cx2 vpc-gen2 cx2.2x4\u2020 2 4GB 4Gbps UBUNTU_18_64 virtual 100GB 0B cx2 vpc-gen2 cx2.32x64 32 64GB 16Gbps UBUNTU_18_64 virtual 100GB 0B cx2 vpc-gen2 cx2.48x96 48 96GB 16Gbps UBUNTU_18_64 virtual 100GB 0B cx2 vpc-gen2 cx2.4x8\u2020 4 8GB 8Gbps UBUNTU_18_64 virtual 100GB 0B cx2 vpc-gen2 cx2.8x16 8 16GB 16Gbps UBUNTU_18_64 virtual 100GB 0B cx2 vpc-gen2 mx2.128x1024 128 1024GB 16Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 mx2.16x128 16 128GB 16Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 mx2.2x16\u2020 2 16GB 4Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 mx2.32x256 32 256GB 16Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 mx2.48x384 48 384GB 16Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 mx2.4x32 4 32GB 8Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 mx2.64x512 64 512GB 16Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 mx2.8x64 8 64GB 16Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 Put the desired flavor in the file. For the OpenShift clusters the command is (https://cloud.ibm.com/docs/containers?topic=containers-clusters); ibmcloud oc flavors --zone ZONE --provider ( classic | vpc-gen2 ) [ --show-storage ] [ --output json ] [ -q ] Example: ibmcloud oc flavors --zone us-south-1 --provider vpc-gen2","title":"Provisionning The Minimal Infrastructure"},{"location":"automation/terraform/2-Provisionning-The-Minimal-Infrastructure/#provisionning-the-minimal-infrastructure","text":"This document describes how to provision the back-end infrastructure for your project. As a prerequisite, Terraform should be installed on the machine used for the following operations. The infrastructure consists on; A \"Virtual Private Cloud\" (VPC) on IBM Cloud. Either an OpenShift or an IKS cluster inside the VPC with all the requirements. A Database for PostgreSQL managed service. An instance of AppID service.","title":"Provisionning The Minimal Infrastructure"},{"location":"automation/terraform/2-Provisionning-The-Minimal-Infrastructure/#setting-up-the-infrastructure-openshift-cluster","text":"The steps to follow to provision the infrastrure are,","title":"Setting up the infrastructure - OpenShift Cluster"},{"location":"automation/terraform/2-Provisionning-The-Minimal-Infrastructure/#step-1-clone-the-following-github-repo","text":"git clone https://github.com/cloud-native-toolkit/iascable `","title":"Step 1: Clone the following Github repo;"},{"location":"automation/terraform/2-Provisionning-The-Minimal-Infrastructure/#step-2-go-to-the-cloned-folder","text":"cd iascable","title":"Step 2: Go to the cloned folder;"},{"location":"automation/terraform/2-Provisionning-The-Minimal-Infrastructure/#step-3-install-the-required-modules-and-packages","text":"npm install and npm run build","title":"Step 3: Install the required modules and packages"},{"location":"automation/terraform/2-Provisionning-The-Minimal-Infrastructure/#step-4-create-the-examplesbaseline-openshiftyaml-yaml-file","text":"apiVersion : cloud.ibm.com/v1alpha1 kind : BillOfMaterial metadata : name : baseline-openshift spec : modules : - name : ibm-resource-group - name : ibm-vpc - name : ibm-vpc-gateways - name : ibm-vpc-subnets alias : cluster-subnets variables : - name : subnet_count value : 1 - name : subnet_label value : cluster - name : ibm-ocp-vpc dependencies : - name : subnets ref : cluster-subnets","title":"Step 4: Create the examples/baseline-openshift.yaml Yaml file;"},{"location":"automation/terraform/2-Provisionning-The-Minimal-Infrastructure/#step-5-log-into-ibm-cloud","text":"ibmcloud login --sso","title":"Step 5: Log into IBM Cloud"},{"location":"automation/terraform/2-Provisionning-The-Minimal-Infrastructure/#step-6-run-the-following-command-from-the-original-folder","text":"./iascable build -i ./examples/baseline-openshift.yaml","title":"Step 6: Run the following command from the original folder;"},{"location":"automation/terraform/2-Provisionning-The-Minimal-Infrastructure/#step-7-edit-the-iascableoutputbaseline-openshiftautotfvars-and-enter-values-for-the-following-parameters","text":"resource_group_name ibmcloud_api_key region name_prefix namespace_name ## resource_group_name: The name of the resource group resource_group_name = \"your-resource-group-name\" ## region: The IBM Cloud region where the cluster will be/has been installed. region = \"eu-de\" (or other IBM Cloud regions) ## ibmcloud_api_key: The IBM Cloud api token ibmcloud_api_key = \"your-ibm-cloud-api-key\" ## namespace_name: The namespace that should be created namespace_name = \"your-namesspace-name\" \u200bSave and quit the file.","title":"Step 7: Edit the \"/iascable/output/baseline-openshift.auto.tfvars\" and enter values for the following parameters;"},{"location":"automation/terraform/2-Provisionning-The-Minimal-Infrastructure/#step-8-edit-the-credentialproperties-file-and-complete-it-as-the-following","text":"# Add the values for the Credentials to access the IBM Cloud # Instructions to access this information can be found in the README.MD classic.username = \"your-ibm-cloud-account-ID\" classic.api.key = \"your-ibm-cloud-classic-api-key\" ibmcloud.api.key = \"your-ibm-cloud-api-key\" # Authentication to OCP can either be performed with username/password or token # If token is provided it will take precedence login.user = \"\" login.password = \"\" login.token = \"\" server.url = \"\" Save and quit the file.","title":"Step 8: Edit the \"credential.properties\" file and complete it as the following;"},{"location":"automation/terraform/2-Provisionning-The-Minimal-Infrastructure/#step-9-open-a-terminal-window-and-run-the-following-command","text":"cd output/baseline-openshift/terraform terraform init terraform plan terraform apply","title":"Step 9: Open a terminal window and run the following command:"},{"location":"automation/terraform/2-Provisionning-The-Minimal-Infrastructure/#setting-up-the-infrastructure-iks-ibm-kubernetes-services-cluster","text":"The steps to follow to provision the infrastrure are, Clone the following Github repo; git clone https://github.com/cloud-native-toolkit/iascable ` Go to the cloned folder; cd iascable Install the required modules and packages npm install and npm run build Create the examples/baseline-iks.yaml Yaml file; apiVersion : cloud.ibm.com/v1alpha1 kind : BillOfMaterial metadata : name : baseline-openshift spec : modules : - name : ibm-resource-group - name : ibm-vpc - name : ibm-vpc-gateways - name : ibm-vpc-subnets alias : cluster-subnets variables : - name : subnet_count value : 1 - name : subnet_label value : cluster - name : ibm-ocp-vpc dependencies : - name : subnets ref : cluster-subnets Log into IBM Cloud ibmcloud login --sso Run the following command from the original folder; ./iascable build -i ./examples/baseline-iks.yaml Edit the \"/iascable/output/baseline-openshift.auto.tfvars\" and enter values for the following parameters; resource_group_name ibmcloud_api_key region name_prefix ## resource_group_name: The name of the resource group resource_group_name = \"your-resource-group-name\" ## region: The IBM Cloud region where the cluster will be/has been installed. region = \"eu-de\" (or other IBM Cloud regions) ## ibmcloud_api_key: The IBM Cloud api token ibmcloud_api_key = \"your-ibm-cloud-api-key\" \u200b Save and quit the file. Edit the \"credential.properties\" file and complete it as the following; # Add the values for the Credentials to access the IBM Cloud # Instructions to access this information can be found in the README.MD classic.username = \"your-ibm-cloud-account-ID\" classic.api.key = \"your-ibm-cloud-classic-api-key\" ibmcloud.api.key = \"your-ibm-cloud-api-key\" # Authentication to OCP can either be performed with username/password or token # If token is provided it will take precedence login.user = \"\" login.password = \"\" login.token = \"\" server.url = \"\" Save and quit the file. Open a terminal window and run the following command: cd output/baseline-openshift/terraform terraform init terraform plan terraform apply","title":"Setting up the infrastructure - IKS (IBM Kubernetes Services) Cluster"},{"location":"automation/terraform/2-Provisionning-The-Minimal-Infrastructure/#changing-some-default-variables","text":"","title":"Changing some default variables"},{"location":"automation/terraform/2-Provisionning-The-Minimal-Infrastructure/#number-of-worker-pools-and-worker-nodes","text":"In both OpenShift and IKS, the defaut configuration provides a cluster with 3 worker pools and 3 worker nodes per pool. If a smaller cluster is needed, for OpenShift the default value can be changed in sub folder \"../baseline-openshift/terraform/variables.tf\" through the \"worker-count\" variable. variable \"worker_count\" { type = number description = \"The number of worker nodes that should be provisioned for classic infrastructure\" default = 3 } For IKS the default value can be changed in sub folder \"../baseline-iks/terraform/variables.tf\" through the \"worker-count\" same variable.","title":"Number of Worker Pools and Worker Nodes"},{"location":"automation/terraform/2-Provisionning-The-Minimal-Infrastructure/#changing-the-default-cluster-flavor","text":"For both OCP/IKS, the default values for the cluster falvor are set in the same \"variables.tf\" as mentioned above. For a change of the cluster flavor, modify the \"cluster_flavor\" variable (https://cloud.ibm.com/docs/containers?topic=containers-clusters). variable \"cluster_flavor\" { type = string description = \"The machine type that will be provisioned for classic infrastructure\" default = \"bx2.4x16\" } To obtain the list of available flavors for IKS clusters proceed as shown; ibmcloud login ( or ibmcloud login --sso if not done already ) ibmcloud target -g <resource_group_name> ibmcloud ks locations Example output: \u276f ibmcloud ks locations VPC Infrastructure Zones Zone Metro Country Geography eu-gb-3 London ( lon ) United Kingdom ( uk ) Europe ( eu ) jp-tok-2 Tokyo ( tok ) Japan ( jp ) Asia Pacific ( ap ) eu-de-1 Frankfurt ( fra ) Germany ( de ) Europe ( eu ) us-east-3 Washington DC ( wdc ) United States ( us ) North America ( na ) ca-tor-3 Toronto ( tor ) Canada ( ca ) North America ( na ) br-sao-3 Sao Paulo ( sao ) Brazil ( br ) South America ( sa ) ca-tor-2 Toronto ( tor ) Canada ( ca ) North America ( na ) jp-osa-2 Osaka ( osa ) Japan ( jp ) Asia Pacific ( ap ) us-east-2 Washington DC ( wdc ) United States ( us ) North America ( na ) au-syd-3 Sydney ( syd ) Australia ( au ) Asia Pacific ( ap ) au-syd-2 Sydney ( syd ) Australia ( au ) Asia Pacific ( ap ) jp-osa-1 Osaka ( osa ) Japan ( jp ) Asia Pacific ( ap ) br-sao-2 Sao Paulo ( sao ) Brazil ( br ) South America ( sa ) eu-gb-1 London ( lon ) United Kingdom ( uk ) Europe ( eu ) eu-de-3 Frankfurt ( fra ) Germany ( de ) Europe ( eu ) jp-tok-3 Tokyo ( tok ) Japan ( jp ) Asia Pacific ( ap ) au-syd-1 Sydney ( syd ) Australia ( au ) Asia Pacific ( ap ) br-sao-1 Sao Paulo ( sao ) Brazil ( br ) South America ( sa ) jp-osa-3 Osaka ( osa ) Japan ( jp ) Asia Pacific ( ap ) us-south-2 Dallas ( dal ) United States ( us ) North America ( na ) jp-tok-1 Tokyo ( tok ) Japan ( jp ) Asia Pacific ( ap ) ca-tor-1 Toronto ( tor ) Canada ( ca ) North America ( na ) eu-gb-2 London ( lon ) United Kingdom ( uk ) Europe ( eu ) us-east-1 Washington DC ( wdc ) United States ( us ) North America ( na ) us-south-1 Dallas ( dal ) United States ( us ) North America ( na ) us-south-3 Dallas ( dal ) United States ( us ) North America ( na ) eu-de-2 Frankfurt ( fra ) Germany ( de ) Europe ( eu ) Classic Infrastructure Zones Zone Metro Country Geography mil01 Milan ( mil ) Italy ( it ) Europe ( eu ) osl01 Oslo ( osl ) Norway ( no ) Europe ( eu ) osa23 Osaka ( osa ) \u2020 Japan ( jp ) Asia Pacific ( ap ) lon06 London ( lon ) \u2020 United Kingdom ( uk ) Europe ( eu ) lon02 London ( lon ) \u2020 United Kingdom ( uk ) Europe ( eu ) che01 Chennai ( che ) India ( in ) Asia Pacific ( ap ) lon04 London ( lon ) \u2020 United Kingdom ( uk ) Europe ( eu ) seo01 Seoul ( seo ) Korea ( kr ) Asia Pacific ( ap ) dal12 Dallas ( dal ) \u2020 United States ( us ) North America ( na ) dal10 Dallas ( dal ) \u2020 United States ( us ) North America ( na ) wdc04 Washington DC ( wdc ) \u2020 United States ( us ) North America ( na ) osa21 Osaka ( osa ) \u2020 Japan ( jp ) Asia Pacific ( ap ) osa22 Osaka ( osa ) \u2020 Japan ( jp ) Asia Pacific ( ap ) sjc03 San Jose ( sjc ) United States ( us ) North America ( na ) mex01 Mexico City ( mex-cty ) Mexico ( mex ) North America ( na ) syd04 Sydney ( syd ) \u2020 Australia ( au ) Asia Pacific ( ap ) hkg02 Hong Kong ( hkg-mtr ) Hong Kong ( hkg ) Asia Pacific ( ap ) mon01 Montreal ( mon ) Canada ( ca ) North America ( na ) tok04 Tokyo ( tok ) \u2020 Japan ( jp ) Asia Pacific ( ap ) par01 Paris ( par ) France ( fr ) Europe ( eu ) syd01 Sydney ( syd ) \u2020 Australia ( au ) Asia Pacific ( ap ) wdc07 Washington DC ( wdc ) \u2020 United States ( us ) North America ( na ) ams03 Amsterdam ( ams ) Netherlands ( nl ) Europe ( eu ) fra04 Frankfurt ( fra ) \u2020 Germany ( de ) Europe ( eu ) tor01 Toronto ( tor ) Canada ( ca ) North America ( na ) fra05 Frankfurt ( fra ) \u2020 Germany ( de ) Europe ( eu ) sjc04 San Jose ( sjc ) United States ( us ) North America ( na ) tok02 Tokyo ( tok ) \u2020 Japan ( jp ) Asia Pacific ( ap ) hou02 Houston ( hou ) United States ( us ) North America ( na ) sao01 Sao Paulo ( sao ) Brazil ( br ) South America ( sa ) lon05 London ( lon ) \u2020 United Kingdom ( uk ) Europe ( eu ) tok05 Tokyo ( tok ) \u2020 Japan ( jp ) Asia Pacific ( ap ) wdc06 Washington DC ( wdc ) \u2020 United States ( us ) North America ( na ) syd05 Sydney ( syd ) \u2020 Australia ( au ) Asia Pacific ( ap ) dal13 Dallas ( dal ) \u2020 United States ( us ) North America ( na ) sng01 Singapore ( sng-mtr ) Singapore ( sng ) Asia Pacific ( ap ) fra02 Frankfurt ( fra ) \u2020 Germany ( de ) Europe ( eu ) \u2020 denotes the zone is in a multizone region. >>ibmcloud ks flavors --zone eu-de-1 For more information about these flavors, see 'https://ibm.biz/flavors' Name Cores Memory Network Speed OS Server Type Storage Secondary Storage Flavor Class Provider bx2.16x64 16 64GB 16Gbps UBUNTU_18_64 virtual 100GB 0B bx2 vpc-gen2 bx2.2x8\u2020 2 8GB 4Gbps UBUNTU_18_64 virtual 100GB 0B bx2 vpc-gen2 bx2.32x128 32 128GB 16Gbps UBUNTU_18_64 virtual 100GB 0B bx2 vpc-gen2 bx2.48x192 48 192GB 16Gbps UBUNTU_18_64 virtual 100GB 0B bx2 vpc-gen2 bx2.4x16 4 16GB 8Gbps UBUNTU_18_64 virtual 100GB 0B bx2 vpc-gen2 bx2.8x32 8 32GB 16Gbps UBUNTU_18_64 virtual 100GB 0B bx2 vpc-gen2 cx2.16x32 16 32GB 16Gbps UBUNTU_18_64 virtual 100GB 0B cx2 vpc-gen2 cx2.2x4\u2020 2 4GB 4Gbps UBUNTU_18_64 virtual 100GB 0B cx2 vpc-gen2 cx2.32x64 32 64GB 16Gbps UBUNTU_18_64 virtual 100GB 0B cx2 vpc-gen2 cx2.48x96 48 96GB 16Gbps UBUNTU_18_64 virtual 100GB 0B cx2 vpc-gen2 cx2.4x8\u2020 4 8GB 8Gbps UBUNTU_18_64 virtual 100GB 0B cx2 vpc-gen2 cx2.8x16 8 16GB 16Gbps UBUNTU_18_64 virtual 100GB 0B cx2 vpc-gen2 mx2.128x1024 128 1024GB 16Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 mx2.16x128 16 128GB 16Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 mx2.2x16\u2020 2 16GB 4Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 mx2.32x256 32 256GB 16Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 mx2.48x384 48 384GB 16Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 mx2.4x32 4 32GB 8Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 mx2.64x512 64 512GB 16Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 mx2.8x64 8 64GB 16Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 Put the desired flavor in the file. For the OpenShift clusters the command is (https://cloud.ibm.com/docs/containers?topic=containers-clusters); ibmcloud oc flavors --zone ZONE --provider ( classic | vpc-gen2 ) [ --show-storage ] [ --output json ] [ -q ] Example: ibmcloud oc flavors --zone us-south-1 --provider vpc-gen2","title":"Changing the default cluster flavor"},{"location":"automation/terraform/3-Provisionning-A-Kubernetes-Based-Infrastructure/","text":"Introduction: Provisionning A Kuberntese Infrastructure \u00b6 This document describes how to provision the back-end infrastructure for your project. As a prerequisite, Terraform should be installed on the machine used for the following operations. The infrastructure consists on; A \"Virtual Private Cloud\" (VPC) on IBM Cloud. Either an OpenShift or an IKS cluster inside the VPC with all the requirements (e.g.: subnets, cidr...). A Database for PostgreSQL managed service (to be implemented). An instance of AppID service (to be implemented). Step 1: Setting up the infrastructure - OpenShift Cluster \u00b6 The steps to follow to provision the infrastrure are, Step 1.1: Clone the following Github repository \u00b6 git clone https://github.com/cloud-native-toolkit/iascable ` Go to the cloned folder; cd iascable Install the required modules and packages npm install and npm run build Step 1.2: Create the examples/baseline-openshift.yaml Yaml file \u00b6 apiVersion : cloud.ibm.com/v1alpha1 kind : BillOfMaterial metadata : name : baseline-openshift spec : modules : - name : ibm-resource-group - name : ibm-vpc - name : ibm-vpc-gateways - name : ibm-vpc-subnets alias : cluster-subnets variables : - name : subnet_count value : 1 - name : subnet_label value : cluster - name : ibm-ocp-vpc dependencies : - name : subnets ref : cluster-subnets Step 1.3: Login into IBM Cloud \u00b6 Log into IBM Cloud ibmcloud login or ibmcloud login --sso Step 1.4: Generate the required YAML file \u00b6 ./iascable build -i ./examples/baseline-openshift.yaml Step 1.5: Set Terraform and properties variables \u00b6 Edit the \"/iascable/output/baseline-openshift.auto.tfvars\" and enter values for the following parameters; resource_group_name ibmcloud_api_key region name_prefix namespace_name ## resource_group_name: The name of the resource group resource_group_name = \"your-resource-group-name\" ## region: The IBM Cloud region where the cluster will be/has been installed. region = \"eu-de\" (or other IBM Cloud regions) ## ibmcloud_api_key: The IBM Cloud api token ibmcloud_api_key = \"your-ibm-cloud-api-key\" ## namespace_name: The namespace that should be created namespace_name = \"your-namesspace-name\" \u200b Save and quit the file. Edit the \"credential.properties\" file and complete it as the following; # Add the values for the Credentials to access the IBM Cloud # Instructions to access this information can be found in the README.MD classic.username = \"your-ibm-cloud-account-ID\" classic.api.key = \"your-ibm-cloud-classic-api-key\" ibmcloud.api.key = \"your-ibm-cloud-api-key\" # Authentication to OCP can either be performed with username/password or token # If token is provided it will take precedence login.user = \"\" login.password = \"\" login.token = \"\" server.url = \"\" Save and quit the file. Step 1.6: Provision the VPC & OpenShift cluster with Terraform \u00b6 Open a terminal window and run the following commands: cd output/baseline-openshift/terraform terraform init terraform plan terraform apply Step 2: Setting up the infrastructure - IKS (IBM Kubernetes Services) Cluster \u00b6 The steps to follow to provision the infrastrure are, Step 2.1: Clone the following Github repository \u00b6 git clone https://github.com/cloud-native-toolkit/iascable ` Go to the cloned folder; cd iascable Install the required modules and packages npm install and npm run build Step 2.2: Create the examples/baseline-iks.yaml Yaml file; \u00b6 apiVersion : cloud.ibm.com/v1alpha1 kind : BillOfMaterial metadata : name : baseline-openshift spec : modules : - name : ibm-resource-group - name : ibm-vpc - name : ibm-vpc-gateways - name : ibm-vpc-subnets alias : cluster-subnets variables : - name : subnet_count value : 1 - name : subnet_label value : cluster - name : ibm-ocp-vpc dependencies : - name : subnets ref : cluster-subnets Step 2.3: Log into IBM Cloud \u00b6 ibmcloud login --sso or ibmcloud login Step 2.4: Generate the required YAML file \u00b6 ./iascable build -i ./examples/baseline-iks.yaml Step 2.5: Set Terraform and properties variables \u00b6 Edit the \"/iascable/output/baseline-iks.auto.tfvars\" and enter values for the following parameters; resource_group_name ibmcloud_api_key region ## resource_group_name: The name of the resource group resource_group_name = \"your-resource-group-name\" ## region: The IBM Cloud region where the cluster will be/has been installed. region = \"eu-de\" (or other IBM Cloud regions) ## ibmcloud_api_key: The IBM Cloud api token ibmcloud_api_key = \"your-ibm-cloud-api-key\" \u200b Save and quit the file. Edit the \"credential.properties\" file and complete it as the following; # Add the values for the Credentials to access the IBM Cloud # Instructions to access this information can be found in the README.MD classic.username = \"your-ibm-cloud-account-ID\" classic.api.key = \"your-ibm-cloud-classic-api-key\" ibmcloud.api.key = \"your-ibm-cloud-api-key\" # Authentication to OCP can either be performed with username/password or token # If token is provided it will take precedence login.user = \"\" login.password = \"\" login.token = \"\" server.url = \"\" Save and quit the file. Step 2.6: Provision the VPC & IKS cluster with Terraform \u00b6 cd output/baseline-openshift/terraform terraform init terraform plan terraform apply VPC & Infrastructure provisioned by Terraform VPC features VPC subnets IKS cluster provisoned with Terraform Step 3: Optional - Changing some default variables \u00b6 Step 3.1: Set the number of Worker Pools and Worker Nodes \u00b6 In both OpenShift and IKS, the defaut configuration provides a cluster with 3 worker pools and 3 worker nodes per pool. If a smaller cluster is needed, for OpenShift the default value can be changed in sub folder \"../baseline-openshift/terraform/variables.tf\" through the \"worker-count\" variable. variable \"worker_count\" { type = number description = \"The number of worker nodes that should be provisioned for classic infrastructure\" default = 3 } For IKS the default value can be changed in sub folder \"../baseline-iks/terraform/variables.tf\" through the \"worker-count\" same variable. Step 3.2: Changing the default cluster flavor \u00b6 For both OCP/IKS, the default values for the cluster falvor are set in the same \"variables.tf\" as mentioned above. For a change of the cluster flavor, modify the \"cluster_flavor\" variable (https://cloud.ibm.com/docs/containers?topic=containers-clusters). variable \"cluster_flavor\" { type = string description = \"The machine type that will be provisioned for classic infrastructure\" default = \"bx2.4x16\" } To obtain the list of available flavors for IKS clusters proceed as follows; ibmcloud login ( or ibmcloud login --sso if not done already ) ibmcloud target -g <resource_group_name> ibmcloud ks locations Example output: \u276f ibmcloud ks locations VPC Infrastructure Zones Zone Metro Country Geography eu-gb-3 London ( lon ) United Kingdom ( uk ) Europe ( eu ) jp-tok-2 Tokyo ( tok ) Japan ( jp ) Asia Pacific ( ap ) eu-de-1 Frankfurt ( fra ) Germany ( de ) Europe ( eu ) us-east-3 Washington DC ( wdc ) United States ( us ) North America ( na ) ca-tor-3 Toronto ( tor ) Canada ( ca ) North America ( na ) br-sao-3 Sao Paulo ( sao ) Brazil ( br ) South America ( sa ) ca-tor-2 Toronto ( tor ) Canada ( ca ) North America ( na ) jp-osa-2 Osaka ( osa ) Japan ( jp ) Asia Pacific ( ap ) us-east-2 Washington DC ( wdc ) United States ( us ) North America ( na ) au-syd-3 Sydney ( syd ) Australia ( au ) Asia Pacific ( ap ) au-syd-2 Sydney ( syd ) Australia ( au ) Asia Pacific ( ap ) jp-osa-1 Osaka ( osa ) Japan ( jp ) Asia Pacific ( ap ) br-sao-2 Sao Paulo ( sao ) Brazil ( br ) South America ( sa ) eu-gb-1 London ( lon ) United Kingdom ( uk ) Europe ( eu ) eu-de-3 Frankfurt ( fra ) Germany ( de ) Europe ( eu ) jp-tok-3 Tokyo ( tok ) Japan ( jp ) Asia Pacific ( ap ) au-syd-1 Sydney ( syd ) Australia ( au ) Asia Pacific ( ap ) br-sao-1 Sao Paulo ( sao ) Brazil ( br ) South America ( sa ) jp-osa-3 Osaka ( osa ) Japan ( jp ) Asia Pacific ( ap ) us-south-2 Dallas ( dal ) United States ( us ) North America ( na ) jp-tok-1 Tokyo ( tok ) Japan ( jp ) Asia Pacific ( ap ) ca-tor-1 Toronto ( tor ) Canada ( ca ) North America ( na ) eu-gb-2 London ( lon ) United Kingdom ( uk ) Europe ( eu ) us-east-1 Washington DC ( wdc ) United States ( us ) North America ( na ) us-south-1 Dallas ( dal ) United States ( us ) North America ( na ) us-south-3 Dallas ( dal ) United States ( us ) North America ( na ) eu-de-2 Frankfurt ( fra ) Germany ( de ) Europe ( eu ) Classic Infrastructure Zones Zone Metro Country Geography mil01 Milan ( mil ) Italy ( it ) Europe ( eu ) osl01 Oslo ( osl ) Norway ( no ) Europe ( eu ) osa23 Osaka ( osa ) \u2020 Japan ( jp ) Asia Pacific ( ap ) lon06 London ( lon ) \u2020 United Kingdom ( uk ) Europe ( eu ) lon02 London ( lon ) \u2020 United Kingdom ( uk ) Europe ( eu ) che01 Chennai ( che ) India ( in ) Asia Pacific ( ap ) lon04 London ( lon ) \u2020 United Kingdom ( uk ) Europe ( eu ) seo01 Seoul ( seo ) Korea ( kr ) Asia Pacific ( ap ) dal12 Dallas ( dal ) \u2020 United States ( us ) North America ( na ) dal10 Dallas ( dal ) \u2020 United States ( us ) North America ( na ) wdc04 Washington DC ( wdc ) \u2020 United States ( us ) North America ( na ) osa21 Osaka ( osa ) \u2020 Japan ( jp ) Asia Pacific ( ap ) osa22 Osaka ( osa ) \u2020 Japan ( jp ) Asia Pacific ( ap ) sjc03 San Jose ( sjc ) United States ( us ) North America ( na ) mex01 Mexico City ( mex-cty ) Mexico ( mex ) North America ( na ) syd04 Sydney ( syd ) \u2020 Australia ( au ) Asia Pacific ( ap ) hkg02 Hong Kong ( hkg-mtr ) Hong Kong ( hkg ) Asia Pacific ( ap ) mon01 Montreal ( mon ) Canada ( ca ) North America ( na ) tok04 Tokyo ( tok ) \u2020 Japan ( jp ) Asia Pacific ( ap ) par01 Paris ( par ) France ( fr ) Europe ( eu ) syd01 Sydney ( syd ) \u2020 Australia ( au ) Asia Pacific ( ap ) wdc07 Washington DC ( wdc ) \u2020 United States ( us ) North America ( na ) ams03 Amsterdam ( ams ) Netherlands ( nl ) Europe ( eu ) fra04 Frankfurt ( fra ) \u2020 Germany ( de ) Europe ( eu ) tor01 Toronto ( tor ) Canada ( ca ) North America ( na ) fra05 Frankfurt ( fra ) \u2020 Germany ( de ) Europe ( eu ) sjc04 San Jose ( sjc ) United States ( us ) North America ( na ) tok02 Tokyo ( tok ) \u2020 Japan ( jp ) Asia Pacific ( ap ) hou02 Houston ( hou ) United States ( us ) North America ( na ) sao01 Sao Paulo ( sao ) Brazil ( br ) South America ( sa ) lon05 London ( lon ) \u2020 United Kingdom ( uk ) Europe ( eu ) tok05 Tokyo ( tok ) \u2020 Japan ( jp ) Asia Pacific ( ap ) wdc06 Washington DC ( wdc ) \u2020 United States ( us ) North America ( na ) syd05 Sydney ( syd ) \u2020 Australia ( au ) Asia Pacific ( ap ) dal13 Dallas ( dal ) \u2020 United States ( us ) North America ( na ) sng01 Singapore ( sng-mtr ) Singapore ( sng ) Asia Pacific ( ap ) fra02 Frankfurt ( fra ) \u2020 Germany ( de ) Europe ( eu ) \u2020 denotes the zone is in a multizone region. >>ibmcloud ks flavors --zone eu-de-1 For more information about these flavors, see 'https://ibm.biz/flavors' Name Cores Memory Network Speed OS Server Type Storage Secondary Storage Flavor Class Provider bx2.16x64 16 64GB 16Gbps UBUNTU_18_64 virtual 100GB 0B bx2 vpc-gen2 bx2.2x8\u2020 2 8GB 4Gbps UBUNTU_18_64 virtual 100GB 0B bx2 vpc-gen2 bx2.32x128 32 128GB 16Gbps UBUNTU_18_64 virtual 100GB 0B bx2 vpc-gen2 bx2.48x192 48 192GB 16Gbps UBUNTU_18_64 virtual 100GB 0B bx2 vpc-gen2 bx2.4x16 4 16GB 8Gbps UBUNTU_18_64 virtual 100GB 0B bx2 vpc-gen2 bx2.8x32 8 32GB 16Gbps UBUNTU_18_64 virtual 100GB 0B bx2 vpc-gen2 cx2.16x32 16 32GB 16Gbps UBUNTU_18_64 virtual 100GB 0B cx2 vpc-gen2 cx2.2x4\u2020 2 4GB 4Gbps UBUNTU_18_64 virtual 100GB 0B cx2 vpc-gen2 cx2.32x64 32 64GB 16Gbps UBUNTU_18_64 virtual 100GB 0B cx2 vpc-gen2 cx2.48x96 48 96GB 16Gbps UBUNTU_18_64 virtual 100GB 0B cx2 vpc-gen2 cx2.4x8\u2020 4 8GB 8Gbps UBUNTU_18_64 virtual 100GB 0B cx2 vpc-gen2 cx2.8x16 8 16GB 16Gbps UBUNTU_18_64 virtual 100GB 0B cx2 vpc-gen2 mx2.128x1024 128 1024GB 16Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 mx2.16x128 16 128GB 16Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 mx2.2x16\u2020 2 16GB 4Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 mx2.32x256 32 256GB 16Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 mx2.48x384 48 384GB 16Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 mx2.4x32 4 32GB 8Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 mx2.64x512 64 512GB 16Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 mx2.8x64 8 64GB 16Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 Put the desired flavor in the file. For the OpenShift clusters the command is (https://cloud.ibm.com/docs/containers?topic=containers-clusters); ibmcloud oc flavors --zone ZONE --provider ( classic | vpc-gen2 ) [ --show-storage ] [ --output json ] [ -q ] Example: ibmcloud oc flavors --zone us-south-1 --provider vpc-gen2","title":"Initial Setup via Scripts"},{"location":"automation/terraform/3-Provisionning-A-Kubernetes-Based-Infrastructure/#introduction-provisionning-a-kuberntese-infrastructure","text":"This document describes how to provision the back-end infrastructure for your project. As a prerequisite, Terraform should be installed on the machine used for the following operations. The infrastructure consists on; A \"Virtual Private Cloud\" (VPC) on IBM Cloud. Either an OpenShift or an IKS cluster inside the VPC with all the requirements (e.g.: subnets, cidr...). A Database for PostgreSQL managed service (to be implemented). An instance of AppID service (to be implemented).","title":"Introduction: Provisionning A Kuberntese Infrastructure"},{"location":"automation/terraform/3-Provisionning-A-Kubernetes-Based-Infrastructure/#step-1-setting-up-the-infrastructure-openshift-cluster","text":"The steps to follow to provision the infrastrure are,","title":"Step 1: Setting up the infrastructure - OpenShift Cluster"},{"location":"automation/terraform/3-Provisionning-A-Kubernetes-Based-Infrastructure/#step-11-clone-the-following-github-repository","text":"git clone https://github.com/cloud-native-toolkit/iascable ` Go to the cloned folder; cd iascable Install the required modules and packages npm install and npm run build","title":"Step 1.1: Clone the following Github repository"},{"location":"automation/terraform/3-Provisionning-A-Kubernetes-Based-Infrastructure/#step-12-create-the-examplesbaseline-openshiftyaml-yaml-file","text":"apiVersion : cloud.ibm.com/v1alpha1 kind : BillOfMaterial metadata : name : baseline-openshift spec : modules : - name : ibm-resource-group - name : ibm-vpc - name : ibm-vpc-gateways - name : ibm-vpc-subnets alias : cluster-subnets variables : - name : subnet_count value : 1 - name : subnet_label value : cluster - name : ibm-ocp-vpc dependencies : - name : subnets ref : cluster-subnets","title":"Step 1.2: Create the examples/baseline-openshift.yaml Yaml file"},{"location":"automation/terraform/3-Provisionning-A-Kubernetes-Based-Infrastructure/#step-13-login-into-ibm-cloud","text":"Log into IBM Cloud ibmcloud login or ibmcloud login --sso","title":"Step 1.3: Login into IBM Cloud"},{"location":"automation/terraform/3-Provisionning-A-Kubernetes-Based-Infrastructure/#step-14-generate-the-required-yaml-file","text":"./iascable build -i ./examples/baseline-openshift.yaml","title":"Step 1.4: Generate the required YAML file"},{"location":"automation/terraform/3-Provisionning-A-Kubernetes-Based-Infrastructure/#step-15-set-terraform-and-properties-variables","text":"Edit the \"/iascable/output/baseline-openshift.auto.tfvars\" and enter values for the following parameters; resource_group_name ibmcloud_api_key region name_prefix namespace_name ## resource_group_name: The name of the resource group resource_group_name = \"your-resource-group-name\" ## region: The IBM Cloud region where the cluster will be/has been installed. region = \"eu-de\" (or other IBM Cloud regions) ## ibmcloud_api_key: The IBM Cloud api token ibmcloud_api_key = \"your-ibm-cloud-api-key\" ## namespace_name: The namespace that should be created namespace_name = \"your-namesspace-name\" \u200b Save and quit the file. Edit the \"credential.properties\" file and complete it as the following; # Add the values for the Credentials to access the IBM Cloud # Instructions to access this information can be found in the README.MD classic.username = \"your-ibm-cloud-account-ID\" classic.api.key = \"your-ibm-cloud-classic-api-key\" ibmcloud.api.key = \"your-ibm-cloud-api-key\" # Authentication to OCP can either be performed with username/password or token # If token is provided it will take precedence login.user = \"\" login.password = \"\" login.token = \"\" server.url = \"\" Save and quit the file.","title":"Step 1.5: Set Terraform and properties variables"},{"location":"automation/terraform/3-Provisionning-A-Kubernetes-Based-Infrastructure/#step-16-provision-the-vpc-openshift-cluster-with-terraform","text":"Open a terminal window and run the following commands: cd output/baseline-openshift/terraform terraform init terraform plan terraform apply","title":"Step 1.6: Provision the VPC &amp; OpenShift cluster with Terraform"},{"location":"automation/terraform/3-Provisionning-A-Kubernetes-Based-Infrastructure/#step-2-setting-up-the-infrastructure-iks-ibm-kubernetes-services-cluster","text":"The steps to follow to provision the infrastrure are,","title":"Step 2: Setting up the infrastructure - IKS (IBM Kubernetes Services) Cluster"},{"location":"automation/terraform/3-Provisionning-A-Kubernetes-Based-Infrastructure/#step-21-clone-the-following-github-repository","text":"git clone https://github.com/cloud-native-toolkit/iascable ` Go to the cloned folder; cd iascable Install the required modules and packages npm install and npm run build","title":"Step 2.1: Clone the following Github repository"},{"location":"automation/terraform/3-Provisionning-A-Kubernetes-Based-Infrastructure/#step-22-create-the-examplesbaseline-iksyaml-yaml-file","text":"apiVersion : cloud.ibm.com/v1alpha1 kind : BillOfMaterial metadata : name : baseline-openshift spec : modules : - name : ibm-resource-group - name : ibm-vpc - name : ibm-vpc-gateways - name : ibm-vpc-subnets alias : cluster-subnets variables : - name : subnet_count value : 1 - name : subnet_label value : cluster - name : ibm-ocp-vpc dependencies : - name : subnets ref : cluster-subnets","title":"Step 2.2: Create the examples/baseline-iks.yaml Yaml file;"},{"location":"automation/terraform/3-Provisionning-A-Kubernetes-Based-Infrastructure/#step-23-log-into-ibm-cloud","text":"ibmcloud login --sso or ibmcloud login","title":"Step 2.3: Log into IBM Cloud"},{"location":"automation/terraform/3-Provisionning-A-Kubernetes-Based-Infrastructure/#step-24-generate-the-required-yaml-file","text":"./iascable build -i ./examples/baseline-iks.yaml","title":"Step 2.4: Generate the required YAML file"},{"location":"automation/terraform/3-Provisionning-A-Kubernetes-Based-Infrastructure/#step-25-set-terraform-and-properties-variables","text":"Edit the \"/iascable/output/baseline-iks.auto.tfvars\" and enter values for the following parameters; resource_group_name ibmcloud_api_key region ## resource_group_name: The name of the resource group resource_group_name = \"your-resource-group-name\" ## region: The IBM Cloud region where the cluster will be/has been installed. region = \"eu-de\" (or other IBM Cloud regions) ## ibmcloud_api_key: The IBM Cloud api token ibmcloud_api_key = \"your-ibm-cloud-api-key\" \u200b Save and quit the file. Edit the \"credential.properties\" file and complete it as the following; # Add the values for the Credentials to access the IBM Cloud # Instructions to access this information can be found in the README.MD classic.username = \"your-ibm-cloud-account-ID\" classic.api.key = \"your-ibm-cloud-classic-api-key\" ibmcloud.api.key = \"your-ibm-cloud-api-key\" # Authentication to OCP can either be performed with username/password or token # If token is provided it will take precedence login.user = \"\" login.password = \"\" login.token = \"\" server.url = \"\" Save and quit the file.","title":"Step 2.5: Set Terraform and properties variables"},{"location":"automation/terraform/3-Provisionning-A-Kubernetes-Based-Infrastructure/#step-26-provision-the-vpc-iks-cluster-with-terraform","text":"cd output/baseline-openshift/terraform terraform init terraform plan terraform apply VPC & Infrastructure provisioned by Terraform VPC features VPC subnets IKS cluster provisoned with Terraform","title":"Step 2.6: Provision the VPC &amp; IKS cluster with Terraform"},{"location":"automation/terraform/3-Provisionning-A-Kubernetes-Based-Infrastructure/#step-3-optional-changing-some-default-variables","text":"","title":"Step 3: Optional - Changing some default variables"},{"location":"automation/terraform/3-Provisionning-A-Kubernetes-Based-Infrastructure/#step-31-set-the-number-of-worker-pools-and-worker-nodes","text":"In both OpenShift and IKS, the defaut configuration provides a cluster with 3 worker pools and 3 worker nodes per pool. If a smaller cluster is needed, for OpenShift the default value can be changed in sub folder \"../baseline-openshift/terraform/variables.tf\" through the \"worker-count\" variable. variable \"worker_count\" { type = number description = \"The number of worker nodes that should be provisioned for classic infrastructure\" default = 3 } For IKS the default value can be changed in sub folder \"../baseline-iks/terraform/variables.tf\" through the \"worker-count\" same variable.","title":"Step 3.1: Set the number of Worker Pools and Worker Nodes"},{"location":"automation/terraform/3-Provisionning-A-Kubernetes-Based-Infrastructure/#step-32-changing-the-default-cluster-flavor","text":"For both OCP/IKS, the default values for the cluster falvor are set in the same \"variables.tf\" as mentioned above. For a change of the cluster flavor, modify the \"cluster_flavor\" variable (https://cloud.ibm.com/docs/containers?topic=containers-clusters). variable \"cluster_flavor\" { type = string description = \"The machine type that will be provisioned for classic infrastructure\" default = \"bx2.4x16\" } To obtain the list of available flavors for IKS clusters proceed as follows; ibmcloud login ( or ibmcloud login --sso if not done already ) ibmcloud target -g <resource_group_name> ibmcloud ks locations Example output: \u276f ibmcloud ks locations VPC Infrastructure Zones Zone Metro Country Geography eu-gb-3 London ( lon ) United Kingdom ( uk ) Europe ( eu ) jp-tok-2 Tokyo ( tok ) Japan ( jp ) Asia Pacific ( ap ) eu-de-1 Frankfurt ( fra ) Germany ( de ) Europe ( eu ) us-east-3 Washington DC ( wdc ) United States ( us ) North America ( na ) ca-tor-3 Toronto ( tor ) Canada ( ca ) North America ( na ) br-sao-3 Sao Paulo ( sao ) Brazil ( br ) South America ( sa ) ca-tor-2 Toronto ( tor ) Canada ( ca ) North America ( na ) jp-osa-2 Osaka ( osa ) Japan ( jp ) Asia Pacific ( ap ) us-east-2 Washington DC ( wdc ) United States ( us ) North America ( na ) au-syd-3 Sydney ( syd ) Australia ( au ) Asia Pacific ( ap ) au-syd-2 Sydney ( syd ) Australia ( au ) Asia Pacific ( ap ) jp-osa-1 Osaka ( osa ) Japan ( jp ) Asia Pacific ( ap ) br-sao-2 Sao Paulo ( sao ) Brazil ( br ) South America ( sa ) eu-gb-1 London ( lon ) United Kingdom ( uk ) Europe ( eu ) eu-de-3 Frankfurt ( fra ) Germany ( de ) Europe ( eu ) jp-tok-3 Tokyo ( tok ) Japan ( jp ) Asia Pacific ( ap ) au-syd-1 Sydney ( syd ) Australia ( au ) Asia Pacific ( ap ) br-sao-1 Sao Paulo ( sao ) Brazil ( br ) South America ( sa ) jp-osa-3 Osaka ( osa ) Japan ( jp ) Asia Pacific ( ap ) us-south-2 Dallas ( dal ) United States ( us ) North America ( na ) jp-tok-1 Tokyo ( tok ) Japan ( jp ) Asia Pacific ( ap ) ca-tor-1 Toronto ( tor ) Canada ( ca ) North America ( na ) eu-gb-2 London ( lon ) United Kingdom ( uk ) Europe ( eu ) us-east-1 Washington DC ( wdc ) United States ( us ) North America ( na ) us-south-1 Dallas ( dal ) United States ( us ) North America ( na ) us-south-3 Dallas ( dal ) United States ( us ) North America ( na ) eu-de-2 Frankfurt ( fra ) Germany ( de ) Europe ( eu ) Classic Infrastructure Zones Zone Metro Country Geography mil01 Milan ( mil ) Italy ( it ) Europe ( eu ) osl01 Oslo ( osl ) Norway ( no ) Europe ( eu ) osa23 Osaka ( osa ) \u2020 Japan ( jp ) Asia Pacific ( ap ) lon06 London ( lon ) \u2020 United Kingdom ( uk ) Europe ( eu ) lon02 London ( lon ) \u2020 United Kingdom ( uk ) Europe ( eu ) che01 Chennai ( che ) India ( in ) Asia Pacific ( ap ) lon04 London ( lon ) \u2020 United Kingdom ( uk ) Europe ( eu ) seo01 Seoul ( seo ) Korea ( kr ) Asia Pacific ( ap ) dal12 Dallas ( dal ) \u2020 United States ( us ) North America ( na ) dal10 Dallas ( dal ) \u2020 United States ( us ) North America ( na ) wdc04 Washington DC ( wdc ) \u2020 United States ( us ) North America ( na ) osa21 Osaka ( osa ) \u2020 Japan ( jp ) Asia Pacific ( ap ) osa22 Osaka ( osa ) \u2020 Japan ( jp ) Asia Pacific ( ap ) sjc03 San Jose ( sjc ) United States ( us ) North America ( na ) mex01 Mexico City ( mex-cty ) Mexico ( mex ) North America ( na ) syd04 Sydney ( syd ) \u2020 Australia ( au ) Asia Pacific ( ap ) hkg02 Hong Kong ( hkg-mtr ) Hong Kong ( hkg ) Asia Pacific ( ap ) mon01 Montreal ( mon ) Canada ( ca ) North America ( na ) tok04 Tokyo ( tok ) \u2020 Japan ( jp ) Asia Pacific ( ap ) par01 Paris ( par ) France ( fr ) Europe ( eu ) syd01 Sydney ( syd ) \u2020 Australia ( au ) Asia Pacific ( ap ) wdc07 Washington DC ( wdc ) \u2020 United States ( us ) North America ( na ) ams03 Amsterdam ( ams ) Netherlands ( nl ) Europe ( eu ) fra04 Frankfurt ( fra ) \u2020 Germany ( de ) Europe ( eu ) tor01 Toronto ( tor ) Canada ( ca ) North America ( na ) fra05 Frankfurt ( fra ) \u2020 Germany ( de ) Europe ( eu ) sjc04 San Jose ( sjc ) United States ( us ) North America ( na ) tok02 Tokyo ( tok ) \u2020 Japan ( jp ) Asia Pacific ( ap ) hou02 Houston ( hou ) United States ( us ) North America ( na ) sao01 Sao Paulo ( sao ) Brazil ( br ) South America ( sa ) lon05 London ( lon ) \u2020 United Kingdom ( uk ) Europe ( eu ) tok05 Tokyo ( tok ) \u2020 Japan ( jp ) Asia Pacific ( ap ) wdc06 Washington DC ( wdc ) \u2020 United States ( us ) North America ( na ) syd05 Sydney ( syd ) \u2020 Australia ( au ) Asia Pacific ( ap ) dal13 Dallas ( dal ) \u2020 United States ( us ) North America ( na ) sng01 Singapore ( sng-mtr ) Singapore ( sng ) Asia Pacific ( ap ) fra02 Frankfurt ( fra ) \u2020 Germany ( de ) Europe ( eu ) \u2020 denotes the zone is in a multizone region. >>ibmcloud ks flavors --zone eu-de-1 For more information about these flavors, see 'https://ibm.biz/flavors' Name Cores Memory Network Speed OS Server Type Storage Secondary Storage Flavor Class Provider bx2.16x64 16 64GB 16Gbps UBUNTU_18_64 virtual 100GB 0B bx2 vpc-gen2 bx2.2x8\u2020 2 8GB 4Gbps UBUNTU_18_64 virtual 100GB 0B bx2 vpc-gen2 bx2.32x128 32 128GB 16Gbps UBUNTU_18_64 virtual 100GB 0B bx2 vpc-gen2 bx2.48x192 48 192GB 16Gbps UBUNTU_18_64 virtual 100GB 0B bx2 vpc-gen2 bx2.4x16 4 16GB 8Gbps UBUNTU_18_64 virtual 100GB 0B bx2 vpc-gen2 bx2.8x32 8 32GB 16Gbps UBUNTU_18_64 virtual 100GB 0B bx2 vpc-gen2 cx2.16x32 16 32GB 16Gbps UBUNTU_18_64 virtual 100GB 0B cx2 vpc-gen2 cx2.2x4\u2020 2 4GB 4Gbps UBUNTU_18_64 virtual 100GB 0B cx2 vpc-gen2 cx2.32x64 32 64GB 16Gbps UBUNTU_18_64 virtual 100GB 0B cx2 vpc-gen2 cx2.48x96 48 96GB 16Gbps UBUNTU_18_64 virtual 100GB 0B cx2 vpc-gen2 cx2.4x8\u2020 4 8GB 8Gbps UBUNTU_18_64 virtual 100GB 0B cx2 vpc-gen2 cx2.8x16 8 16GB 16Gbps UBUNTU_18_64 virtual 100GB 0B cx2 vpc-gen2 mx2.128x1024 128 1024GB 16Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 mx2.16x128 16 128GB 16Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 mx2.2x16\u2020 2 16GB 4Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 mx2.32x256 32 256GB 16Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 mx2.48x384 48 384GB 16Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 mx2.4x32 4 32GB 8Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 mx2.64x512 64 512GB 16Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 mx2.8x64 8 64GB 16Gbps UBUNTU_18_64 virtual 100GB 0B mx2 vpc-gen2 Put the desired flavor in the file. For the OpenShift clusters the command is (https://cloud.ibm.com/docs/containers?topic=containers-clusters); ibmcloud oc flavors --zone ZONE --provider ( classic | vpc-gen2 ) [ --show-storage ] [ --output json ] [ -q ] Example: ibmcloud oc flavors --zone us-south-1 --provider vpc-gen2","title":"Step 3.2: Changing the default cluster flavor"},{"location":"creation-of-managed-ibm-cloud-services/configure-appid/","text":"Configure AppId \u00b6 ------------------ UNDER CONSTRUCTION ------------------ In this section we will explain scripts which can be used to configure the AppId service instance, so that it can be used by the sample frontend application.","title":"Programmatic Configuration of AppID"},{"location":"creation-of-managed-ibm-cloud-services/configure-appid/#configure-appid","text":"------------------ UNDER CONSTRUCTION ------------------ In this section we will explain scripts which can be used to configure the AppId service instance, so that it can be used by the sample frontend application.","title":"Configure AppId"},{"location":"creation-of-managed-ibm-cloud-services/create-appid/","text":"Create AppId \u00b6 ------------------ UNDER CONSTRUCTION ------------------ In this section we will explain scripts which can be used to create a AppId service instance, required for both CodeEngine or k8s deployments.","title":"Programmatic Creation of AppID"},{"location":"creation-of-managed-ibm-cloud-services/create-appid/#create-appid","text":"------------------ UNDER CONSTRUCTION ------------------ In this section we will explain scripts which can be used to create a AppId service instance, required for both CodeEngine or k8s deployments.","title":"Create AppId"},{"location":"creation-of-managed-ibm-cloud-services/create-postgres-schema/","text":"Create Postgres Schema \u00b6 ------------------ UNDER CONSTRUCTION ------------------ In this section we will explain scripts which can be used to create the Postgres schema required by the sample application.","title":"Programmatic Configuration of Postgres including Schema"},{"location":"creation-of-managed-ibm-cloud-services/create-postgres-schema/#create-postgres-schema","text":"------------------ UNDER CONSTRUCTION ------------------ In this section we will explain scripts which can be used to create the Postgres schema required by the sample application.","title":"Create Postgres Schema"},{"location":"creation-of-managed-ibm-cloud-services/create-postgres/","text":"Create Postgres \u00b6 ------------------ UNDER CONSTRUCTION ------------------ In this section we will explain scripts which can be used to create a Postgres service instance, required for both CodeEngine or k8s deployments.","title":"Programmatic Creation of Postgres"},{"location":"creation-of-managed-ibm-cloud-services/create-postgres/#create-postgres","text":"------------------ UNDER CONSTRUCTION ------------------ In this section we will explain scripts which can be used to create a Postgres service instance, required for both CodeEngine or k8s deployments.","title":"Create Postgres"},{"location":"development_of_microservices/authentication-flow-appip-backend-frontend/","text":"Authentication Flow (AppID, backend, frontend) \u00b6 The following diagram display a simplified authentication flow in the serverless environment. The numbering visualizes the simplified invocation sequence. The web application URL is invoked by an user in the browser and the browser loads the web application from the Nginx server. The web application redirects the user to the App ID login page and he inserts his ID and password. App ID validates the user and the application invocation. App ID provides an id token and access token in a JWT format. The web application uses the access token to invoke the secured backend endpoint. The backend verifies if the token is valid. The backend access securely the Postgres database with a user, password and certificate information.","title":"Authentication Flow (AppID, backend, frontend)"},{"location":"development_of_microservices/authentication-flow-appip-backend-frontend/#authentication-flow-appid-backend-frontend","text":"The following diagram display a simplified authentication flow in the serverless environment. The numbering visualizes the simplified invocation sequence. The web application URL is invoked by an user in the browser and the browser loads the web application from the Nginx server. The web application redirects the user to the App ID login page and he inserts his ID and password. App ID validates the user and the application invocation. App ID provides an id token and access token in a JWT format. The web application uses the access token to invoke the secured backend endpoint. The backend verifies if the token is valid. The backend access securely the Postgres database with a user, password and certificate information.","title":"Authentication Flow (AppID, backend, frontend)"},{"location":"development_of_microservices/backend-service-container/","text":"Backend Microservice Container \u00b6 Quarkus is a great way to build microservices with Java. It doesn't require a lot of memory, it starts very fast, it comes with very popular Java libraries and has a big community. When creating new Quarkus projects via the CLI, Maven or the UI, various Dockerfiles are created automatically. In this project the generated Dockerfile is used which is based on ubi8/ubi-minimal and uses the Hotspot JVM. Check out the Dockerfile . Description: The image has not been optimized, for example by using OpenJ9. The generated Dockerfile is roughly from mid 2020 and uses old versions. As a result the IBM vulnerability scans show errors which need to be fixed by using newer versions. The image doesn't use root users so that it can be run on OpenShift. The Dockerfile uses two stages. The first stage uses Maven to build the application. This stage should be updated as well. The image listens to port 8081. The image can be configured with various environment variables to access Postgres and AppID. See the deployment.yaml for details. The Postgres certificate is not copied on the image for security reasons. Instead it is copied on the container directory '/cloud-postgres-cert' via a bash script .","title":"Quarkus Backend Service Container"},{"location":"development_of_microservices/backend-service-container/#backend-microservice-container","text":"Quarkus is a great way to build microservices with Java. It doesn't require a lot of memory, it starts very fast, it comes with very popular Java libraries and has a big community. When creating new Quarkus projects via the CLI, Maven or the UI, various Dockerfiles are created automatically. In this project the generated Dockerfile is used which is based on ubi8/ubi-minimal and uses the Hotspot JVM. Check out the Dockerfile . Description: The image has not been optimized, for example by using OpenJ9. The generated Dockerfile is roughly from mid 2020 and uses old versions. As a result the IBM vulnerability scans show errors which need to be fixed by using newer versions. The image doesn't use root users so that it can be run on OpenShift. The Dockerfile uses two stages. The first stage uses Maven to build the application. This stage should be updated as well. The image listens to port 8081. The image can be configured with various environment variables to access Postgres and AppID. See the deployment.yaml for details. The Postgres certificate is not copied on the image for security reasons. Instead it is copied on the container directory '/cloud-postgres-cert' via a bash script .","title":"Backend Microservice Container"},{"location":"development_of_microservices/backend-service-impl/","text":"Implementation of the Backend Microservice \u00b6 You can find the implementation of the backend service in the multi-tenancy-backend repo. The service has been developed with Quarkus: Dependencies managed via Maven REST Endpoints via Rest Easy http://localhost:8081/category is protected and will return a response code '401' not authorized http://localhost:8081/category/2/products is not protected and will return data from Postgres Authentication Implementation Persistence via Hibernate The backend service uses different databases for different tenants as well as different AppID service instances The Postgres and AppID instances need to be created first and they need to be configured (e.g. test users, sample data) Configuration of Postgres and AppID Check out these local development instructions how to develop, run and debug the backend and frontend services locally The application runs on port 8081 CORS is enabled","title":"Quarkus Backend Service Code"},{"location":"development_of_microservices/backend-service-impl/#implementation-of-the-backend-microservice","text":"You can find the implementation of the backend service in the multi-tenancy-backend repo. The service has been developed with Quarkus: Dependencies managed via Maven REST Endpoints via Rest Easy http://localhost:8081/category is protected and will return a response code '401' not authorized http://localhost:8081/category/2/products is not protected and will return data from Postgres Authentication Implementation Persistence via Hibernate The backend service uses different databases for different tenants as well as different AppID service instances The Postgres and AppID instances need to be created first and they need to be configured (e.g. test users, sample data) Configuration of Postgres and AppID Check out these local development instructions how to develop, run and debug the backend and frontend services locally The application runs on port 8081 CORS is enabled","title":"Implementation of the Backend Microservice"},{"location":"development_of_microservices/externalization-of-variables-in-backend-microservices/","text":"Externalization of Variables in Backend Microservices \u00b6 We need to configure for our backend microservice following information in externalized variables. We need to securely connect to the Postgres database Postgres certificate Postgres connection url Postgres user and password We need to configure the OpenID connect information for Quarkus For the basics about the externalization of variables, you can also visit that blog post called HOW TO USE ENVIRONMENT VARIABLES TO MAKE A CONTAINERIZED QUARKUS APPLICATION MORE FLEXIBLE","title":"Externalization of Variables in Backend Microservices"},{"location":"development_of_microservices/externalization-of-variables-in-backend-microservices/#externalization-of-variables-in-backend-microservices","text":"We need to configure for our backend microservice following information in externalized variables. We need to securely connect to the Postgres database Postgres certificate Postgres connection url Postgres user and password We need to configure the OpenID connect information for Quarkus For the basics about the externalization of variables, you can also visit that blog post called HOW TO USE ENVIRONMENT VARIABLES TO MAKE A CONTAINERIZED QUARKUS APPLICATION MORE FLEXIBLE","title":"Externalization of Variables in Backend Microservices"},{"location":"development_of_microservices/externalization-of-variables-in-frontend-microservices/","text":"Externalization of Variables in Frontend Microservices \u00b6 We need to configure for our fontend microservice following information in externalized variables. We need to connect to App ID Discoveryendpoint Client ID We need to connect to the backend URLs to the backend We need to configure the frontend name and basic navigation Headline Category name For the basics about the externalization of variables, you can also visit that blog post called USE ENVIRONMENT VARIABLES TO MAKE A CONTAINERIZED VUE.JS FRONTEND APPLICATION MORE FLEXIBLE AND DEPLOY IT TO CODE ENGINE","title":"Externalization of Variables in Frontend Microservices"},{"location":"development_of_microservices/externalization-of-variables-in-frontend-microservices/#externalization-of-variables-in-frontend-microservices","text":"We need to configure for our fontend microservice following information in externalized variables. We need to connect to App ID Discoveryendpoint Client ID We need to connect to the backend URLs to the backend We need to configure the frontend name and basic navigation Headline Category name For the basics about the externalization of variables, you can also visit that blog post called USE ENVIRONMENT VARIABLES TO MAKE A CONTAINERIZED VUE.JS FRONTEND APPLICATION MORE FLEXIBLE AND DEPLOY IT TO CODE ENGINE","title":"Externalization of Variables in Frontend Microservices"},{"location":"development_of_microservices/frontend-access-the-backend-endpoint/","text":"Access the backend endpoint \u00b6 When we going to invoke the backend endpoint, we need to ensure: We are logged on We have a valid access token We invoke the backend microservice endpoint using axios Here is one example invokation of the backend microservice endpoint. ... import axios from \"axios\" ; ... export default { name : \"app\" , components : { Catalog }, computed : { isAuthenticated () { return this . $store . state . user . isAuthenticated ; } }, ... methods : { readProducts ( categoryId , categoryName ) { console . log ( \"--> log readProducts.categoryId: \" , categoryId ); var logURL = this . apiUrlProducts + \"/\" + categoryId + \"/products\" ; console . log ( \"--> log readProducts.categoryId: \" , categoryId ); console . log ( \"--> log readProducts: URL \" , logURL ); this . categoryName = categoryName ; if ( this . loadingProducts == false ) { this . loadingProducts = true ; const axiosService = axios . create ({ timeout : 30000 , headers : { \"Content-Type\" : \"application/json\" , Authorization : \"Bearer \" + this . $store . state . user . accessToken } }); let that = this ; axiosService . get ( this . apiUrlProducts + \"/\" + categoryId + \"/products\" ) . then ( function ( response ) { console . log ( \"--> log: Product data : \" + JSON . stringify ( response . data )); that . loadingProducts = false ; that . error = \"\" ; that . $store . commit ( \"addProducts\" , response . data ); }) . catch ( function ( e ) { var error = \"--> log: Can't load products: \" + e ; that . loadingProducts = false ; console . error ( error ); that . errorLoadingProducts = error ; that . $store . commit ( \"logout\" ); }); } }, },","title":"Access the backend endpoint"},{"location":"development_of_microservices/frontend-access-the-backend-endpoint/#access-the-backend-endpoint","text":"When we going to invoke the backend endpoint, we need to ensure: We are logged on We have a valid access token We invoke the backend microservice endpoint using axios Here is one example invokation of the backend microservice endpoint. ... import axios from \"axios\" ; ... export default { name : \"app\" , components : { Catalog }, computed : { isAuthenticated () { return this . $store . state . user . isAuthenticated ; } }, ... methods : { readProducts ( categoryId , categoryName ) { console . log ( \"--> log readProducts.categoryId: \" , categoryId ); var logURL = this . apiUrlProducts + \"/\" + categoryId + \"/products\" ; console . log ( \"--> log readProducts.categoryId: \" , categoryId ); console . log ( \"--> log readProducts: URL \" , logURL ); this . categoryName = categoryName ; if ( this . loadingProducts == false ) { this . loadingProducts = true ; const axiosService = axios . create ({ timeout : 30000 , headers : { \"Content-Type\" : \"application/json\" , Authorization : \"Bearer \" + this . $store . state . user . accessToken } }); let that = this ; axiosService . get ( this . apiUrlProducts + \"/\" + categoryId + \"/products\" ) . then ( function ( response ) { console . log ( \"--> log: Product data : \" + JSON . stringify ( response . data )); that . loadingProducts = false ; that . error = \"\" ; that . $store . commit ( \"addProducts\" , response . data ); }) . catch ( function ( e ) { var error = \"--> log: Can't load products: \" + e ; that . loadingProducts = false ; console . error ( error ); that . errorLoadingProducts = error ; that . $store . commit ( \"logout\" ); }); } }, },","title":"Access the backend endpoint"},{"location":"development_of_microservices/frontend-service-code/","text":"Frontend service code \u00b6 The frontend web client displays a catalog. You need to authenticate and then you can navigate to that catalog. The data for the catalog is provided by a backend microservice. The frontend needs to be integrated with App ID for the authentication and authorization to invoke the backend microservice. The frontend is build on following components/frameworks. Vue.js to build the frontend Material Design to define the content elements of the website like buttons menus and so on. App ID client JavaScript SDK for single WebPage to setup the integration for the App ID service usage. axios to invoke HTTPS endpoints. Useful to know: How to use axios to invoke HTTPS endpoints? How to implement await in JavaScript , when we use the App ID JavaScript client SDK? How to get the User with the App ID client JavaScript SDK?","title":"Vue.js Frontend Service Code"},{"location":"development_of_microservices/frontend-service-code/#frontend-service-code","text":"The frontend web client displays a catalog. You need to authenticate and then you can navigate to that catalog. The data for the catalog is provided by a backend microservice. The frontend needs to be integrated with App ID for the authentication and authorization to invoke the backend microservice. The frontend is build on following components/frameworks. Vue.js to build the frontend Material Design to define the content elements of the website like buttons menus and so on. App ID client JavaScript SDK for single WebPage to setup the integration for the App ID service usage. axios to invoke HTTPS endpoints. Useful to know: How to use axios to invoke HTTPS endpoints? How to implement await in JavaScript , when we use the App ID JavaScript client SDK? How to get the User with the App ID client JavaScript SDK?","title":"Frontend service code"},{"location":"development_of_microservices/frontend-service-container/","text":"Frontend service container \u00b6 The first point is we need to recall the very basics for a web frontend client. A frontend client runs in a browser. That means the all the html, css and scripts needs to be loaded in the browser and the browser is the runtime environment for the web frontend client. Yes, that's sounds simple, but sometimes we tend to forget it. Challenges are in that context: How to transfer the static files? How to secure the container during the execution? How to handle changing endpoints? How to transfer the static files? \u00b6 Based on that fact we need a way to transfer all the static files for to browser as fast as possible over the internet. Also we need to keep in mind we want to provide our frontend later as a container, so that our application can be executed on different container runtime platforms such as serverless Code Engine, Kubernetes, OpenShift and so on. With all this in mind we need to chose an effective web server, we have chosen the open source version of Ngnix do to that job. How to secure the container during the execution? \u00b6 Ngnix is fast, but by default it needs to run as root when it listen to port 80 (additional information on stackoverflow ). So we configured the Docker container image to as non root. Our your user is called nginxuser . This is an extract of the Dockerfile for the frontend application. # Add a user how will have the rights to change the files in code RUN addgroup -g 1500 nginxusers RUN adduser --disabled-password -u 1501 nginxuser nginxusers How to handle changing endpoints? \u00b6 The next topic is how to set variables for an application which doesn't run on the server you control? Because, when our container will start on different platforms the URLs for App ID and the backend will changing? The way to handle this is the externalization of variables, so that we can provide the container during the startup the needed information. How that works is one of the additional topics in our documentation.","title":"Vue.js Frontend Service Container"},{"location":"development_of_microservices/frontend-service-container/#frontend-service-container","text":"The first point is we need to recall the very basics for a web frontend client. A frontend client runs in a browser. That means the all the html, css and scripts needs to be loaded in the browser and the browser is the runtime environment for the web frontend client. Yes, that's sounds simple, but sometimes we tend to forget it. Challenges are in that context: How to transfer the static files? How to secure the container during the execution? How to handle changing endpoints?","title":"Frontend service container"},{"location":"development_of_microservices/frontend-service-container/#how-to-transfer-the-static-files","text":"Based on that fact we need a way to transfer all the static files for to browser as fast as possible over the internet. Also we need to keep in mind we want to provide our frontend later as a container, so that our application can be executed on different container runtime platforms such as serverless Code Engine, Kubernetes, OpenShift and so on. With all this in mind we need to chose an effective web server, we have chosen the open source version of Ngnix do to that job.","title":"How to transfer the static files?"},{"location":"development_of_microservices/frontend-service-container/#how-to-secure-the-container-during-the-execution","text":"Ngnix is fast, but by default it needs to run as root when it listen to port 80 (additional information on stackoverflow ). So we configured the Docker container image to as non root. Our your user is called nginxuser . This is an extract of the Dockerfile for the frontend application. # Add a user how will have the rights to change the files in code RUN addgroup -g 1500 nginxusers RUN adduser --disabled-password -u 1501 nginxuser nginxusers","title":"How to secure the container during the execution?"},{"location":"development_of_microservices/frontend-service-container/#how-to-handle-changing-endpoints","text":"The next topic is how to set variables for an application which doesn't run on the server you control? Because, when our container will start on different platforms the URLs for App ID and the backend will changing? The way to handle this is the externalization of variables, so that we can provide the container during the startup the needed information. How that works is one of the additional topics in our documentation.","title":"How to handle changing endpoints?"},{"location":"development_of_microservices/frontend-use-of-the-appid-javascript-sdk/","text":"Frontend use of the App ID client JavaScript SDK \u00b6 This part of the documentation give you a basic understand how the App ID client SDK is integrated to Frontend. The full functionality of the WebClient will be only available after a successful authenication Use the App ID client SDK in Vue.js \u00b6 Relevant code in the main.js file. The code is structured in: Set variable for authentication Functions Login ( appID.Signin() ) Renew ( appID.silentSignin() ) -> no longer used App ID authentication init Create Vue.js application instance import AppID from 'ibmcloud-appid-js' ; ... /**********************************/ /* Set variable for authentication /**********************************/ let appid_init ; let user_info ; /**********************************/ /* Functions /**********************************/ async function asyncAppIDInit ( appID ) { var appID_init_Result = await appID . init ( initOptions ); console . log ( \"--> log: appID_init_Result \" , appID_init_Result ); /**********************************/ /* Check if the user is already authenticated /**********************************/ if ( ! store . state . user . isAuthenticated ) { try { /******************************/ /* Authentication /******************************/ let tokens = await appID . signin (); console . log ( \"--> log: tokens \" , tokens ); user_info = { isAuthenticated : true , idToken : tokens . idToken , accessToken : tokens . accessToken , name : tokens . idTokenPayload . given_name } store . commit ( \"login\" , user_info ); return true ; } catch ( e ) { console . log ( \"--> log: error \" , e ); return false ; } } } async function asyncAppIDrefresh ( appID ) { if ( store . state . user . isAuthenticated == true ) { try { /******************************/ /* Authentication /******************************/ let tokens = await appID . silentSignin (); console . log ( \"--> log: silentSignin tokens \" , tokens ); user_info = { isAuthenticated : true , idToken : tokens . idToken , accessToken : tokens . accessToken // name : tokens.idTokenPayload.given_name } store . commit ( \"login\" , user_info ); return true ; } catch ( e ) { console . log ( \"--> log: catch interval error \" , e ); return false ; } } else { console . log ( \"--> log: no refresh \" ); return false ; } } /**********************************/ /* App ID authentication init /**********************************/ appid_init = { //web-app-tenant-a-single appid_clientId : window . VUE_APPID_CLIENT_ID , appid_discoveryEndpoint : window . VUE_APPID_DISCOVERYENDPOINT } console . log ( \"--> log: appid_init\" , appid_init ); store . commit ( \"setAppID\" , appid_init ); let initOptions = { clientId : store . state . appid_init . appid_clientId , discoveryEndpoint : store . state . appid_init . appid_discoveryEndpoint } /**********************************/ /* Create vue application instance /**********************************/ let appID = new AppID (); let init_messsage = \"\" ; if ( ! ( init_messsage = asyncAppIDInit ( appID ))) { console . log ( \"--> log: init_messsage : \" + init_messsage ); window . location . reload (); } else { console . log ( \"--> log: init_messsage : \" + init_messsage ); // Vue application instance new Vue ({ store , router , render : h => h ( App ) }). $mount ( '#app' ) } /**********************************/ /* App ID authentication renew_token with silentSignin /**********************************/ let renew_token ; setInterval (() => { console . log ( \"--> log: token interval \" ); console . log ( \"--> log: isAuthenticated \" , store . state . user . isAuthenticated ); if ( store . state . user . isAuthenticated == false ) { renew_token = asyncAppIDrefresh ( appID ); console . log ( \"--> log: renew_token : \" + renew_token ); } else { console . log ( \"--> log: renew_token : \" + renew_token ); user_info = { isAuthenticated : false , idToken : \" \" , accessToken : \" \" , name : \" \" } store . commit ( \"login\" , user_info ); } }, 10000 ); Add the App ID client SDK to the Vue.js application \u00b6 Given resources: App ID client SDK for Single WebPage How to implement await in JavaScript Client SDK JavaScript npm install ibmcloud-appid-js Note: To use refresh token, you need to enable refresh token, as you see in the image below.","title":"Frontend use of the App ID client JavaScript SDK"},{"location":"development_of_microservices/frontend-use-of-the-appid-javascript-sdk/#frontend-use-of-the-app-id-client-javascript-sdk","text":"This part of the documentation give you a basic understand how the App ID client SDK is integrated to Frontend. The full functionality of the WebClient will be only available after a successful authenication","title":"Frontend use of the App ID client JavaScript SDK"},{"location":"development_of_microservices/frontend-use-of-the-appid-javascript-sdk/#use-the-app-id-client-sdk-in-vuejs","text":"Relevant code in the main.js file. The code is structured in: Set variable for authentication Functions Login ( appID.Signin() ) Renew ( appID.silentSignin() ) -> no longer used App ID authentication init Create Vue.js application instance import AppID from 'ibmcloud-appid-js' ; ... /**********************************/ /* Set variable for authentication /**********************************/ let appid_init ; let user_info ; /**********************************/ /* Functions /**********************************/ async function asyncAppIDInit ( appID ) { var appID_init_Result = await appID . init ( initOptions ); console . log ( \"--> log: appID_init_Result \" , appID_init_Result ); /**********************************/ /* Check if the user is already authenticated /**********************************/ if ( ! store . state . user . isAuthenticated ) { try { /******************************/ /* Authentication /******************************/ let tokens = await appID . signin (); console . log ( \"--> log: tokens \" , tokens ); user_info = { isAuthenticated : true , idToken : tokens . idToken , accessToken : tokens . accessToken , name : tokens . idTokenPayload . given_name } store . commit ( \"login\" , user_info ); return true ; } catch ( e ) { console . log ( \"--> log: error \" , e ); return false ; } } } async function asyncAppIDrefresh ( appID ) { if ( store . state . user . isAuthenticated == true ) { try { /******************************/ /* Authentication /******************************/ let tokens = await appID . silentSignin (); console . log ( \"--> log: silentSignin tokens \" , tokens ); user_info = { isAuthenticated : true , idToken : tokens . idToken , accessToken : tokens . accessToken // name : tokens.idTokenPayload.given_name } store . commit ( \"login\" , user_info ); return true ; } catch ( e ) { console . log ( \"--> log: catch interval error \" , e ); return false ; } } else { console . log ( \"--> log: no refresh \" ); return false ; } } /**********************************/ /* App ID authentication init /**********************************/ appid_init = { //web-app-tenant-a-single appid_clientId : window . VUE_APPID_CLIENT_ID , appid_discoveryEndpoint : window . VUE_APPID_DISCOVERYENDPOINT } console . log ( \"--> log: appid_init\" , appid_init ); store . commit ( \"setAppID\" , appid_init ); let initOptions = { clientId : store . state . appid_init . appid_clientId , discoveryEndpoint : store . state . appid_init . appid_discoveryEndpoint } /**********************************/ /* Create vue application instance /**********************************/ let appID = new AppID (); let init_messsage = \"\" ; if ( ! ( init_messsage = asyncAppIDInit ( appID ))) { console . log ( \"--> log: init_messsage : \" + init_messsage ); window . location . reload (); } else { console . log ( \"--> log: init_messsage : \" + init_messsage ); // Vue application instance new Vue ({ store , router , render : h => h ( App ) }). $mount ( '#app' ) } /**********************************/ /* App ID authentication renew_token with silentSignin /**********************************/ let renew_token ; setInterval (() => { console . log ( \"--> log: token interval \" ); console . log ( \"--> log: isAuthenticated \" , store . state . user . isAuthenticated ); if ( store . state . user . isAuthenticated == false ) { renew_token = asyncAppIDrefresh ( appID ); console . log ( \"--> log: renew_token : \" + renew_token ); } else { console . log ( \"--> log: renew_token : \" + renew_token ); user_info = { isAuthenticated : false , idToken : \" \" , accessToken : \" \" , name : \" \" } store . commit ( \"login\" , user_info ); } }, 10000 );","title":"Use the App ID client SDK in Vue.js"},{"location":"development_of_microservices/frontend-use-of-the-appid-javascript-sdk/#add-the-app-id-client-sdk-to-the-vuejs-application","text":"Given resources: App ID client SDK for Single WebPage How to implement await in JavaScript Client SDK JavaScript npm install ibmcloud-appid-js Note: To use refresh token, you need to enable refresh token, as you see in the image below.","title":"Add the App ID client SDK to the Vue.js application"},{"location":"development_of_microservices/local-development/","text":"Develop Backend Service locally \u00b6 To run the backend service locally, a managed Postgres instance needs to be created first. After this you need to define four variables in local.env. See local.env.template for more: POSTGRES_USERNAME POSTGRES_PASSWORD POSTGRES_URL POSTGRES_CERTIFICATE_FILE_NAME Additionally you need to copy the certificate file in ./src/main/resources/certificates. As file name use the Postgres username. For the authentication a App ID instance is required. Copy the two settings in local.env: APPID_CLIENT_ID (note: this is not the client id in the secrets, but in the application settings) APPID_DISCOVERYENDPOINT For IBMers only: You can re-use existing services by using these configuration files. git clone https://github.com/IBM/multi-tenancy.git git clone https://github.com/IBM/multi-tenancy-backend.git git clone https://github.com/IBM/multi-tenancy-frontend.git cd multi-tenancy ROOT_FOLDER = $( pwd ) cp certificate ${ root_folder } /src/main/resources/certificates/ cp template.local.env local.env vi local.env Backend \u00b6 Run the backend service locally via Maven: sh ./scripts/run-locally-backend.sh Or run the backend service locally via container (podman): sh ./scripts/run-locally-container-backend.sh Invoke http://localhost:8081/category/2/products Frontend \u00b6 Run the frontend service locally: sh ./scripts/run-locally-frontend.sh Or run the frontend service locally via container (podman): sh ./scripts/run-locally-container-frontend.sh Invoke http://localhost:8080 User: thomas@example.com. Password: thomas4appid","title":"Local Development of Services"},{"location":"development_of_microservices/local-development/#develop-backend-service-locally","text":"To run the backend service locally, a managed Postgres instance needs to be created first. After this you need to define four variables in local.env. See local.env.template for more: POSTGRES_USERNAME POSTGRES_PASSWORD POSTGRES_URL POSTGRES_CERTIFICATE_FILE_NAME Additionally you need to copy the certificate file in ./src/main/resources/certificates. As file name use the Postgres username. For the authentication a App ID instance is required. Copy the two settings in local.env: APPID_CLIENT_ID (note: this is not the client id in the secrets, but in the application settings) APPID_DISCOVERYENDPOINT For IBMers only: You can re-use existing services by using these configuration files. git clone https://github.com/IBM/multi-tenancy.git git clone https://github.com/IBM/multi-tenancy-backend.git git clone https://github.com/IBM/multi-tenancy-frontend.git cd multi-tenancy ROOT_FOLDER = $( pwd ) cp certificate ${ root_folder } /src/main/resources/certificates/ cp template.local.env local.env vi local.env","title":"Develop Backend Service locally"},{"location":"development_of_microservices/local-development/#backend","text":"Run the backend service locally via Maven: sh ./scripts/run-locally-backend.sh Or run the backend service locally via container (podman): sh ./scripts/run-locally-container-backend.sh Invoke http://localhost:8081/category/2/products","title":"Backend"},{"location":"development_of_microservices/local-development/#frontend","text":"Run the frontend service locally: sh ./scripts/run-locally-frontend.sh Or run the frontend service locally via container (podman): sh ./scripts/run-locally-container-frontend.sh Invoke http://localhost:8080 User: thomas@example.com. Password: thomas4appid","title":"Frontend"},{"location":"k8s/3-ci-cd/README_cd/","text":"Create DevSecOps pipelines to update a Kubernetes application \u00b6 DevSecOps integrates a set of security and compliance controls into the DevOps processes, allowing frequent code delivery while maintaining a strong security posture and continuous state of audit-readiness. This setion steps you through the creation of Continuous Integration (CD) toolchains using an IBM Cloud toolchain templates, which we will customize to deploy the sample application to an IBM Cloud Kubernetes cluster. Before you begin \u00b6 Set up the following pre-requisites: Ensure you have cloned the following repos to your GitHub account: https://github.com/IBM/multi-tenancy https://github.com/IBM/multi-tenancy-backend https://github.com/IBM/multi-tenancy-frontend An instance of the IBM Cloud Secrets Manager service (the same one you used for CI pipelines) An IBM Cloud Kubernetes Service cluster or an IBM Cloud OpenShift cluster (the same one you used for CI pipelines) Create a namespace in the IBM Cloud Container Registry (the same one you used for CI pipelines) Create the toolchain \u00b6 Click Create Toolchain and select the DevSecOps filter: Create CD toolchain \u00b6 Click the CD tile to launch the setup wizard, and complete the fields by refering to the following screenshots (refering to your own GitHub repos): Create Dev Mode trigger & Update Environmental Properties for CD \u00b6 The environmental properties for the CD pipeline should be updated as follows (using your own GitHub multi-tenancy/backend/frontend repos): Note that the Text fields pipeline-config-branch and target-environment were updated. Additional Text Fields entitled branch , multi-tenancy-frontend , multi-tenancy-backend , multi-tenancy and tenant were added. An additional Tool Integration field entitled repository was added. When setting this field, you must specify a JSON filter of parameters.repo_url . Select the Trigger tab. You may need to correct the Git CD Trigger if it shows a hazard symbol: Edit its properties and select a valid the branch name: Also, copy the existing Manual CD Trigger and use it to create a Manual CD Trigger force redeploy trigger which can be used to bypass some elements of the toolchain, for testing purposes. Note that two properties have been added: Testing the CD pipeline (Optional) \u00b6 In this section, you can make a quick test of the CD pipeline. For a more detailed explanation of how to use the pipelines to onboard a new tenant, see the Overview section. First you'll need to execute the Manual Promotion Trigger . This creates a new branch and a merge request in the GitLab Inventory Repository. You should approve that merge request before proceeding. You can find a link to the merge request in the output of the pipeline run: Essential information must be added to the merge request. Click the Edit button and complete the Change Request assignee and Priority as follows, then Save Changes : You may also create an EMERGENCY label on the right hand panel, but you do not need to assign the label to this merge request, although it may be useful in future. Approval is optional, simply press the blue Merge button. You can now trigger the pipeline with a Manual CD Trigger . Click the pipeline run name to view the progress. After a few minutes, a successful result should look like this: In the logs for the prod-deployment run-stage you will find two outputs starting Application URL: . These are the URLs to test the backend and frontend applications. Give them a try! (The username and password to login to the frontend is User: thomas@example.com. Password: thomas4appid)","title":"CD Toolchains"},{"location":"k8s/3-ci-cd/README_cd/#create-devsecops-pipelines-to-update-a-kubernetes-application","text":"DevSecOps integrates a set of security and compliance controls into the DevOps processes, allowing frequent code delivery while maintaining a strong security posture and continuous state of audit-readiness. This setion steps you through the creation of Continuous Integration (CD) toolchains using an IBM Cloud toolchain templates, which we will customize to deploy the sample application to an IBM Cloud Kubernetes cluster.","title":"Create DevSecOps pipelines to update a Kubernetes application"},{"location":"k8s/3-ci-cd/README_cd/#before-you-begin","text":"Set up the following pre-requisites: Ensure you have cloned the following repos to your GitHub account: https://github.com/IBM/multi-tenancy https://github.com/IBM/multi-tenancy-backend https://github.com/IBM/multi-tenancy-frontend An instance of the IBM Cloud Secrets Manager service (the same one you used for CI pipelines) An IBM Cloud Kubernetes Service cluster or an IBM Cloud OpenShift cluster (the same one you used for CI pipelines) Create a namespace in the IBM Cloud Container Registry (the same one you used for CI pipelines)","title":"Before you begin"},{"location":"k8s/3-ci-cd/README_cd/#create-the-toolchain","text":"Click Create Toolchain and select the DevSecOps filter:","title":"Create the toolchain"},{"location":"k8s/3-ci-cd/README_cd/#create-cd-toolchain","text":"Click the CD tile to launch the setup wizard, and complete the fields by refering to the following screenshots (refering to your own GitHub repos):","title":"Create CD toolchain"},{"location":"k8s/3-ci-cd/README_cd/#create-dev-mode-trigger-update-environmental-properties-for-cd","text":"The environmental properties for the CD pipeline should be updated as follows (using your own GitHub multi-tenancy/backend/frontend repos): Note that the Text fields pipeline-config-branch and target-environment were updated. Additional Text Fields entitled branch , multi-tenancy-frontend , multi-tenancy-backend , multi-tenancy and tenant were added. An additional Tool Integration field entitled repository was added. When setting this field, you must specify a JSON filter of parameters.repo_url . Select the Trigger tab. You may need to correct the Git CD Trigger if it shows a hazard symbol: Edit its properties and select a valid the branch name: Also, copy the existing Manual CD Trigger and use it to create a Manual CD Trigger force redeploy trigger which can be used to bypass some elements of the toolchain, for testing purposes. Note that two properties have been added:","title":"Create Dev Mode trigger &amp; Update Environmental Properties for CD"},{"location":"k8s/3-ci-cd/README_cd/#testing-the-cd-pipeline-optional","text":"In this section, you can make a quick test of the CD pipeline. For a more detailed explanation of how to use the pipelines to onboard a new tenant, see the Overview section. First you'll need to execute the Manual Promotion Trigger . This creates a new branch and a merge request in the GitLab Inventory Repository. You should approve that merge request before proceeding. You can find a link to the merge request in the output of the pipeline run: Essential information must be added to the merge request. Click the Edit button and complete the Change Request assignee and Priority as follows, then Save Changes : You may also create an EMERGENCY label on the right hand panel, but you do not need to assign the label to this merge request, although it may be useful in future. Approval is optional, simply press the blue Merge button. You can now trigger the pipeline with a Manual CD Trigger . Click the pipeline run name to view the progress. After a few minutes, a successful result should look like this: In the logs for the prod-deployment run-stage you will find two outputs starting Application URL: . These are the URLs to test the backend and frontend applications. Give them a try! (The username and password to login to the frontend is User: thomas@example.com. Password: thomas4appid)","title":"Testing the CD pipeline (Optional)"},{"location":"k8s/3-ci-cd/README_ci/","text":"Create DevSecOps pipelines to update a Kubernetes application \u00b6 DevSecOps integrates a set of security and compliance controls into the DevOps processes, allowing frequent code delivery while maintaining a strong security posture and continuous state of audit-readiness. This setion steps you through the creation of Continuous Integration (CI) toolchains using an IBM Cloud toolchain templates, which we will customize to deploy the sample application to an IBM Cloud Kuberenets cluster. Before you begin \u00b6 Set up the following pre-requisites: Ensure you have cloned the following repos to your GitHub account: https://github.com/IBM/multi-tenancy https://github.com/IBM/multi-tenancy-backend https://github.com/IBM/multi-tenancy-frontend An instance of the IBM Cloud Secrets Manager service An IBM Cloud Kubernetes Service cluster or an IBM Cloud OpenShift cluster Create a namespace in the IBM Cloud Container Registry (access via hamburger menu->Container Registry) Create Postgres and AppId service instances, and service API keys. See section Creation of managed IBM Cloud Services Create an IBM Cloud API key in Secrets Manager \u00b6 In IBM Cloud, navigate to Manage->Access (IAM)->API Keys. Create a new IBM Cloud API Key and make a note of it. In your Secrets Manager service, create a new Arbitary value and paste in the IBM Cloud API Key: Update the sample application configuration \u00b6 In your clone of https://github.com/IBM/multi-tenancy, navigate to the folder configuration/tenants. This is the configuration file for each tenant of the SaaS application which the toolchains will deploy. You should update both tenant-a.json and tenant-b.json with the name of the IKS cluster you are using for testing, and commit the changes: N.B. the wizard to create the toolchains (below) also requests the name of the kubernetes cluster, but the resulting toolchain environmental property is not used by this sample. Also note the definition of SERVICE_KEY_NAME fields for the AppId and Postgres cloud services. If you have previously deployed the sample code to serverless, the deployment scripts should have created appropriate API keys. In folder configuration/global.json, update the values for the RESOURCE_GROUP and REGION where you created your Kubernetes cluster, and the container registry NAMESPACE you created for the container images: It is not necessary to change the IMAGES section. Create the CI Backend toolchain \u00b6 Login to IBM Cloud and use the hamburger menu in the top left to navigate to DevOps . Select the Resource Group and Region thenh Create Toolchain and select the DevSecOps filter: Click the CI tile to launch the setup wizard, and complete the fields by refering to the following screenshots (refering to your own GitHub repos): The CI pipeline for backend also requires access to the multi-tenancy repostory, in addition to the multi-tenancy-backend repostory which has already been configured by the wizard. Click the blue Add tool button and manually add the GitHub repostory for multi-tenancy by following these steps: Add or update the toolchain's Environmental Properties as follows: Note that the Text fields pipeline-config-branch and opt-in-sonar need to be modified. An additional Text Fields entitled branch , multi-tenancy-frontend , multi-tenancy-backend and multi-tenancy were added. An additional Tool Integration field entitled repository was added. When setting this field, you must specify a JSON filter of parameters.repo_url : Select the Trigger tab. Note the Git CI Trigger shows a hazard symbol. Edit its properties and select a valid the branch name: Create Dev Mode trigger & Update Environmental Properties for CI Frontend \u00b6 Enable a Dev mode trigger which permits a faster pipeline run which does not invoke all compliance steps. Select the CI pipeline tile, then Trigger. Duplicate the existing Manual Trigger and set the properties as follows: Add or update the Environmental Properties as follows: Note that the Text fields pipeline-config-branch and opt-in-sonar were updated. Additional Text Fields entitled branch , multi-tenancy-frontend , multi-tenancy-backend and multi-tenancy were added. An additional Tool Integration field entitled repository was added. When setting this field, you must specify a JSON filter of parameters.repo_url : Select the Trigger tab. Note the Git CI Trigger shows a hazard symbol. Edit its properties and select a valid the branch name: Create CI toolchain for frontend \u00b6 Login to IBM Cloud and use the hamburger menu in the top left to navigate to DevOps . Select the Resource Group and Region thenh Create Toolchain and select the DevSecOps filter: Click the CI tile to launch the setup wizard, and complete the fields by refering to the following screenshots (refering to your own GitHub repos): The CI pipeline for frontend also requires access to the multi-tenancy repostory, in addition to the multi-tenancy-backend repostory which has already been configured by the wizard. Click the blue Add tool button and manually add the GitHub repostory for multi-tenancy by following these steps: Create Dev Mode trigger & Update Environmental Properties for CI Backend \u00b6 Enable a Dev mode trigger which permits a faster pipeline run which does not invoke all compliance steps. Select the CI pipeline tile, then Trigger. Duplicate the existing Manual Trigger and set the properties as follows: Add or update the toolchain's Environmental Properties as follows: Note that the Text fields pipeline-config-branch and opt-in-sonar need to be modified. An additional Text Fields entitled branch , multi-tenancy-frontend , multi-tenancy-backend and multi-tenancy were added. An additional Tool Integration field entitled repository was added. When setting this field, you must specify a JSON filter of parameters.repo_url : Select the Trigger tab. Note the Git CI Trigger shows a hazard symbol. Edit its properties and select a valid the branch name: Testing the CI pipelines (Optional) \u00b6 In this section, you can make a quick test of the CI pipelines. For a more detailed explanation of how to use the pipelines to onboard a new tenant, see the Overview section. First test the backend CI pipelines by triggering a pipeline run, using Manual Trigger . Click the pipeline run name to view the progress. After a few minutes, a successful result should look like this: Test the front end CI pipeline by triggering a pipeline run, using Manual Trigger . Click the pipeline run name to view the progress. After a few minutes, a successful result should look like this: If all steps complete successfully, you can configure and test the CD Toolchain.","title":"CI Toolchains"},{"location":"k8s/3-ci-cd/README_ci/#create-devsecops-pipelines-to-update-a-kubernetes-application","text":"DevSecOps integrates a set of security and compliance controls into the DevOps processes, allowing frequent code delivery while maintaining a strong security posture and continuous state of audit-readiness. This setion steps you through the creation of Continuous Integration (CI) toolchains using an IBM Cloud toolchain templates, which we will customize to deploy the sample application to an IBM Cloud Kuberenets cluster.","title":"Create DevSecOps pipelines to update a Kubernetes application"},{"location":"k8s/3-ci-cd/README_ci/#before-you-begin","text":"Set up the following pre-requisites: Ensure you have cloned the following repos to your GitHub account: https://github.com/IBM/multi-tenancy https://github.com/IBM/multi-tenancy-backend https://github.com/IBM/multi-tenancy-frontend An instance of the IBM Cloud Secrets Manager service An IBM Cloud Kubernetes Service cluster or an IBM Cloud OpenShift cluster Create a namespace in the IBM Cloud Container Registry (access via hamburger menu->Container Registry) Create Postgres and AppId service instances, and service API keys. See section Creation of managed IBM Cloud Services","title":"Before you begin"},{"location":"k8s/3-ci-cd/README_ci/#create-an-ibm-cloud-api-key-in-secrets-manager","text":"In IBM Cloud, navigate to Manage->Access (IAM)->API Keys. Create a new IBM Cloud API Key and make a note of it. In your Secrets Manager service, create a new Arbitary value and paste in the IBM Cloud API Key:","title":"Create an IBM Cloud API key in Secrets Manager"},{"location":"k8s/3-ci-cd/README_ci/#update-the-sample-application-configuration","text":"In your clone of https://github.com/IBM/multi-tenancy, navigate to the folder configuration/tenants. This is the configuration file for each tenant of the SaaS application which the toolchains will deploy. You should update both tenant-a.json and tenant-b.json with the name of the IKS cluster you are using for testing, and commit the changes: N.B. the wizard to create the toolchains (below) also requests the name of the kubernetes cluster, but the resulting toolchain environmental property is not used by this sample. Also note the definition of SERVICE_KEY_NAME fields for the AppId and Postgres cloud services. If you have previously deployed the sample code to serverless, the deployment scripts should have created appropriate API keys. In folder configuration/global.json, update the values for the RESOURCE_GROUP and REGION where you created your Kubernetes cluster, and the container registry NAMESPACE you created for the container images: It is not necessary to change the IMAGES section.","title":"Update the sample application configuration"},{"location":"k8s/3-ci-cd/README_ci/#create-the-ci-backend-toolchain","text":"Login to IBM Cloud and use the hamburger menu in the top left to navigate to DevOps . Select the Resource Group and Region thenh Create Toolchain and select the DevSecOps filter: Click the CI tile to launch the setup wizard, and complete the fields by refering to the following screenshots (refering to your own GitHub repos): The CI pipeline for backend also requires access to the multi-tenancy repostory, in addition to the multi-tenancy-backend repostory which has already been configured by the wizard. Click the blue Add tool button and manually add the GitHub repostory for multi-tenancy by following these steps: Add or update the toolchain's Environmental Properties as follows: Note that the Text fields pipeline-config-branch and opt-in-sonar need to be modified. An additional Text Fields entitled branch , multi-tenancy-frontend , multi-tenancy-backend and multi-tenancy were added. An additional Tool Integration field entitled repository was added. When setting this field, you must specify a JSON filter of parameters.repo_url : Select the Trigger tab. Note the Git CI Trigger shows a hazard symbol. Edit its properties and select a valid the branch name:","title":"Create the CI Backend toolchain"},{"location":"k8s/3-ci-cd/README_ci/#create-dev-mode-trigger-update-environmental-properties-for-ci-frontend","text":"Enable a Dev mode trigger which permits a faster pipeline run which does not invoke all compliance steps. Select the CI pipeline tile, then Trigger. Duplicate the existing Manual Trigger and set the properties as follows: Add or update the Environmental Properties as follows: Note that the Text fields pipeline-config-branch and opt-in-sonar were updated. Additional Text Fields entitled branch , multi-tenancy-frontend , multi-tenancy-backend and multi-tenancy were added. An additional Tool Integration field entitled repository was added. When setting this field, you must specify a JSON filter of parameters.repo_url : Select the Trigger tab. Note the Git CI Trigger shows a hazard symbol. Edit its properties and select a valid the branch name:","title":"Create Dev Mode trigger &amp; Update Environmental Properties for CI Frontend"},{"location":"k8s/3-ci-cd/README_ci/#create-ci-toolchain-for-frontend","text":"Login to IBM Cloud and use the hamburger menu in the top left to navigate to DevOps . Select the Resource Group and Region thenh Create Toolchain and select the DevSecOps filter: Click the CI tile to launch the setup wizard, and complete the fields by refering to the following screenshots (refering to your own GitHub repos): The CI pipeline for frontend also requires access to the multi-tenancy repostory, in addition to the multi-tenancy-backend repostory which has already been configured by the wizard. Click the blue Add tool button and manually add the GitHub repostory for multi-tenancy by following these steps:","title":"Create CI toolchain for frontend"},{"location":"k8s/3-ci-cd/README_ci/#create-dev-mode-trigger-update-environmental-properties-for-ci-backend","text":"Enable a Dev mode trigger which permits a faster pipeline run which does not invoke all compliance steps. Select the CI pipeline tile, then Trigger. Duplicate the existing Manual Trigger and set the properties as follows: Add or update the toolchain's Environmental Properties as follows: Note that the Text fields pipeline-config-branch and opt-in-sonar need to be modified. An additional Text Fields entitled branch , multi-tenancy-frontend , multi-tenancy-backend and multi-tenancy were added. An additional Tool Integration field entitled repository was added. When setting this field, you must specify a JSON filter of parameters.repo_url : Select the Trigger tab. Note the Git CI Trigger shows a hazard symbol. Edit its properties and select a valid the branch name:","title":"Create Dev Mode trigger &amp; Update Environmental Properties for CI Backend"},{"location":"k8s/3-ci-cd/README_ci/#testing-the-ci-pipelines-optional","text":"In this section, you can make a quick test of the CI pipelines. For a more detailed explanation of how to use the pipelines to onboard a new tenant, see the Overview section. First test the backend CI pipelines by triggering a pipeline run, using Manual Trigger . Click the pipeline run name to view the progress. After a few minutes, a successful result should look like this: Test the front end CI pipeline by triggering a pipeline run, using Manual Trigger . Click the pipeline run name to view the progress. After a few minutes, a successful result should look like this: If all steps complete successfully, you can configure and test the CD Toolchain.","title":"Testing the CI pipelines (Optional)"},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/cd-pipeline/","text":"IBM DevSecOps Reference Implementation - CD Pipeline \u00b6 The CD pipeline deploys the application to the production environments of specific tenants. For compliance reasons it needs to be triggered manually and the promotion pipeline needs to be run before. Step 1: Trigger manually the CD pipeline for certain tenant. \u00b6 Step 2: The CI pipeline reads the configuration. Either Kubernetes or OpenShift can be used; in a shared cluster or isolated clusters for tentants. \u00b6 The configuration is read: Step 3: Repos are cloned: \u00b6 Step 4: The delta is calculated, since only changes are deployed. Additionally security checks are performed again. \u00b6 Step 5: The actual deployment is performed. \u00b6 Step 6: The application can be opened. \u00b6 Step 7: Data is collected. \u00b6","title":"CD pipeline"},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/cd-pipeline/#ibm-devsecops-reference-implementation-cd-pipeline","text":"The CD pipeline deploys the application to the production environments of specific tenants. For compliance reasons it needs to be triggered manually and the promotion pipeline needs to be run before.","title":"IBM DevSecOps Reference Implementation - CD Pipeline"},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/cd-pipeline/#step-1-trigger-manually-the-cd-pipeline-for-certain-tenant","text":"","title":"Step 1: Trigger manually the CD pipeline for certain tenant."},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/cd-pipeline/#step-2-the-ci-pipeline-reads-the-configuration-either-kubernetes-or-openshift-can-be-used-in-a-shared-cluster-or-isolated-clusters-for-tentants","text":"The configuration is read:","title":"Step 2: The CI pipeline reads the configuration. Either Kubernetes or OpenShift can be used; in a shared cluster or isolated clusters for tentants."},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/cd-pipeline/#step-3-repos-are-cloned","text":"","title":"Step 3: Repos are cloned:"},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/cd-pipeline/#step-4-the-delta-is-calculated-since-only-changes-are-deployed-additionally-security-checks-are-performed-again","text":"","title":"Step 4: The delta is calculated, since only changes are deployed. Additionally security checks are performed again."},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/cd-pipeline/#step-5-the-actual-deployment-is-performed","text":"","title":"Step 5: The actual deployment is performed."},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/cd-pipeline/#step-6-the-application-can-be-opened","text":"","title":"Step 6: The application can be opened."},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/cd-pipeline/#step-7-data-is-collected","text":"","title":"Step 7: Data is collected."},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/cd-pull-request/","text":"IBM DevSecOps Reference Implementation - CD Pull Request \u00b6 In order to deploy a new version for a specific tenant, a pull request (which is the same as a merge request in GitLab) has to be created and merged. The pull request merges the latest version in the main branch of the inventory to the tenant specific branches in the inventory. Step 1: Trigger manually the CD promotion trigger pipeline. \u00b6 Step 2: A pull request is created. \u00b6 Step 3: In the pull request the priority and assignee has to be defined. After this it can be saved and merged. \u00b6 Next After the pull request has been merged, the actual CD pipeline can be triggered.","title":"CI pull request"},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/cd-pull-request/#ibm-devsecops-reference-implementation-cd-pull-request","text":"In order to deploy a new version for a specific tenant, a pull request (which is the same as a merge request in GitLab) has to be created and merged. The pull request merges the latest version in the main branch of the inventory to the tenant specific branches in the inventory.","title":"IBM DevSecOps Reference Implementation - CD Pull Request"},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/cd-pull-request/#step-1-trigger-manually-the-cd-promotion-trigger-pipeline","text":"","title":"Step 1: Trigger manually the CD promotion trigger pipeline."},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/cd-pull-request/#step-2-a-pull-request-is-created","text":"","title":"Step 2: A pull request is created."},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/cd-pull-request/#step-3-in-the-pull-request-the-priority-and-assignee-has-to-be-defined-after-this-it-can-be-saved-and-merged","text":"Next After the pull request has been merged, the actual CD pipeline can be triggered.","title":"Step 3: In the pull request the priority and assignee has to be defined. After this it can be saved and merged."},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/ci-pipeline/","text":"IBM DevSecOps Reference Implementation - CI Pipeline \u00b6 The CI pipelines (one for backend, one for frontend) build and push the images and run various security and code tests. Only if all checks pass, the application can be deployed to production via the CD pipelines. This assures that new versions can be deployed at any time based on business (not technical) decisions. Overview: Build and push images Run various security checks (secret detection, image vulnerabilities, compliance) Run various code tests (unit tests, acceptance tests) Deploy services to integration/testing Kubernetes namespaces or OpenShift projects Step 1: The CI pipeline is triggered automatically after the pull request has been merged. \u00b6 Step 2: The CI pipeline reads the configuration. \u00b6 Step 3: The image is built and pushed. \u00b6 Step 4: The backend container is deployed to an integration/testing Kubernetes namespace or OpenShift project. \u00b6 Step 5: The status can be monitored in IBM DevOps Insights. \u00b6 Step 6: The latest successful version is stored in the inventory repo. \u00b6 Step 7: Evidence is collected in the evidence repo \u00b6 Step 8: If the pipeline run has been successful, no issues are created in the compliance issues repo. \u00b6 Next: After a successful run of the CI pipeline, the CD pipeline can be run. \u00b6","title":"CI pipeline"},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/ci-pipeline/#ibm-devsecops-reference-implementation-ci-pipeline","text":"The CI pipelines (one for backend, one for frontend) build and push the images and run various security and code tests. Only if all checks pass, the application can be deployed to production via the CD pipelines. This assures that new versions can be deployed at any time based on business (not technical) decisions. Overview: Build and push images Run various security checks (secret detection, image vulnerabilities, compliance) Run various code tests (unit tests, acceptance tests) Deploy services to integration/testing Kubernetes namespaces or OpenShift projects","title":"IBM DevSecOps Reference Implementation - CI Pipeline"},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/ci-pipeline/#step-1-the-ci-pipeline-is-triggered-automatically-after-the-pull-request-has-been-merged","text":"","title":"Step 1: The CI pipeline is triggered automatically after the pull request has been merged."},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/ci-pipeline/#step-2-the-ci-pipeline-reads-the-configuration","text":"","title":"Step 2: The CI pipeline reads the configuration."},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/ci-pipeline/#step-3-the-image-is-built-and-pushed","text":"","title":"Step 3: The image is built and pushed."},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/ci-pipeline/#step-4-the-backend-container-is-deployed-to-an-integrationtesting-kubernetes-namespace-or-openshift-project","text":"","title":"Step 4: The backend container is deployed to an integration/testing Kubernetes namespace or OpenShift project."},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/ci-pipeline/#step-5-the-status-can-be-monitored-in-ibm-devops-insights","text":"","title":"Step 5: The status can be monitored in IBM DevOps Insights."},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/ci-pipeline/#step-6-the-latest-successful-version-is-stored-in-the-inventory-repo","text":"","title":"Step 6: The latest successful version is stored in the inventory repo."},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/ci-pipeline/#step-7-evidence-is-collected-in-the-evidence-repo","text":"","title":"Step 7: Evidence is collected in the evidence repo"},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/ci-pipeline/#step-8-if-the-pipeline-run-has-been-successful-no-issues-are-created-in-the-compliance-issues-repo","text":"","title":"Step 8: If the pipeline run has been successful, no issues are created in the compliance issues repo."},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/ci-pipeline/#next-after-a-successful-run-of-the-ci-pipeline-the-cd-pipeline-can-be-run","text":"","title":"Next: After a successful run of the CI pipeline, the CD pipeline  can be run."},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/ci-pull-request/","text":"IBM DevSecOps Reference Implementation - CI Pull Request \u00b6 Before developers can push their code into 'main', security checks need to pass and approvals need to be done first. Step 1: A developer creates a new version of README.md in the backend repo. The change is done in a developer branch. \u00b6 Step 2: The developer creates a pull request. \u00b6 Step 3: Before the pull request can be merged, security checks are performed via the 'backend pr-pipeline'. \u00b6 Step 4: After the security checks have passed, an approval from a second developer is required. \u00b6 Step 5: The second developer approves the pull request. \u00b6 Step 6: The pull request can now be merged. \u00b6 Next: When the pull request has been merged, it triggers the CI pipeline . \u00b6","title":"CI pull request"},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/ci-pull-request/#ibm-devsecops-reference-implementation-ci-pull-request","text":"Before developers can push their code into 'main', security checks need to pass and approvals need to be done first.","title":"IBM DevSecOps Reference Implementation - CI Pull Request"},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/ci-pull-request/#step-1-a-developer-creates-a-new-version-of-readmemd-in-the-backend-repo-the-change-is-done-in-a-developer-branch","text":"","title":"Step 1: A developer creates a new version of README.md in the backend repo. The change is done in a developer branch."},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/ci-pull-request/#step-2-the-developer-creates-a-pull-request","text":"","title":"Step 2: The developer creates a pull request."},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/ci-pull-request/#step-3-before-the-pull-request-can-be-merged-security-checks-are-performed-via-the-backend-pr-pipeline","text":"","title":"Step 3: Before the pull request can be merged, security checks are performed via the 'backend pr-pipeline'."},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/ci-pull-request/#step-4-after-the-security-checks-have-passed-an-approval-from-a-second-developer-is-required","text":"","title":"Step 4: After the security checks have passed, an approval from a second developer is required."},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/ci-pull-request/#step-5-the-second-developer-approves-the-pull-request","text":"","title":"Step 5: The second developer approves the pull request."},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/ci-pull-request/#step-6-the-pull-request-can-now-be-merged","text":"","title":"Step 6: The pull request can now be merged."},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/ci-pull-request/#next-when-the-pull-request-has-been-merged-it-triggers-the-ci-pipeline","text":"","title":"Next: When the pull request has been merged, it triggers the CI pipeline ."},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/devsecops-overview/","text":"IBM DevSecOps Reference Implementation \u00b6 IBM uses internally a common practice and process how to implement DevSecOps. This process has been published so that IBM clients, partners and developers can use it too. See DevSecOps Reference Implementation for Audit-Ready Compliance for details. This project leverages the DevSecOps reference architecture to deploy the backend and frontend containers to Kubernetes and OpenShift on the IBM Cloud. There is IBM Cloud documentation that describes how to set up the CI and CD toolchains. The documentation below adopts this information for the multi-tenancy sample application. Overview \u00b6 There are six steps to deploy the SaaS sample application. CI backend pull request: Get approval to merge backend code changes into main CI backend pipeline: Pipeline to build backend image and deploy it to a staging environment for testing CI frontend pull request: Get approval to merge frontend code changes into main CI frontend pipeline: Pipeline to build frontend image and deploy it to a staging environment for testing CD pull request: Pull request to deploy main for a specific tenant to production CD pipeline: After approval deploy backend and frontend for a specific tenant to production Check out the following documents for details: CI pull request related to (1) and (3) CI pipeline related to (2) and (4) CD pull request related to (5) CD pipeline related to (6) Toolchains \u00b6 There are three toolchains: CI for backend CI for frontend CD Repos \u00b6 These three repos contain the code of the microservices and the toolchains (on github.com): multi-tenancy multi-tenancy-backend multi-tenancy-frontend The following repos contain state information and are shared by the three toolchains (on IBM Cloud GitLab): inventory complicance change management compliance issues evidence The following repo contains the code for the 'out-of-the-box' security checks (on IBM Cloud GitLab): compliance pipelines","title":"Overview"},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/devsecops-overview/#ibm-devsecops-reference-implementation","text":"IBM uses internally a common practice and process how to implement DevSecOps. This process has been published so that IBM clients, partners and developers can use it too. See DevSecOps Reference Implementation for Audit-Ready Compliance for details. This project leverages the DevSecOps reference architecture to deploy the backend and frontend containers to Kubernetes and OpenShift on the IBM Cloud. There is IBM Cloud documentation that describes how to set up the CI and CD toolchains. The documentation below adopts this information for the multi-tenancy sample application.","title":"IBM DevSecOps Reference Implementation"},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/devsecops-overview/#overview","text":"There are six steps to deploy the SaaS sample application. CI backend pull request: Get approval to merge backend code changes into main CI backend pipeline: Pipeline to build backend image and deploy it to a staging environment for testing CI frontend pull request: Get approval to merge frontend code changes into main CI frontend pipeline: Pipeline to build frontend image and deploy it to a staging environment for testing CD pull request: Pull request to deploy main for a specific tenant to production CD pipeline: After approval deploy backend and frontend for a specific tenant to production Check out the following documents for details: CI pull request related to (1) and (3) CI pipeline related to (2) and (4) CD pull request related to (5) CD pipeline related to (6)","title":"Overview"},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/devsecops-overview/#toolchains","text":"There are three toolchains: CI for backend CI for frontend CD","title":"Toolchains"},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/devsecops-overview/#repos","text":"These three repos contain the code of the microservices and the toolchains (on github.com): multi-tenancy multi-tenancy-backend multi-tenancy-frontend The following repos contain state information and are shared by the three toolchains (on IBM Cloud GitLab): inventory complicance change management compliance issues evidence The following repo contains the code for the 'out-of-the-box' security checks (on IBM Cloud GitLab): compliance pipelines","title":"Repos"},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/k8s-billing/","text":"OpenShift/IBM Kubernetes Service Billing \u00b6 ------------------ UNDER CONSTRUCTION ------------------ In this section we will explain techniques which can be used to track the relative resource consumption of individual namespaces/projects, when each tenant is deployed to a shared OpenShift/IBM Kubernetes cluster. We will describe how to achieve this on a per namespace/project level with IBM Cloud Monitoring . As all tenants share the same OpenShift/IBM Kubernetes cluster, standard IBM Cloud billing can be used to see this fixed cost. Each tenant will have their own service instances for Postgres and AppId, so we will show how standard IBM Cloud billing can be used to see costs per service instance (or resource group, if we decide to organise the services in this way). If however the business prefers to use Postgres in a multi-tenant way, we will also describe how to use IBM Cloud Monitoring to monitor Postgres 'per shema'.","title":"Billing"},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/k8s-billing/#openshiftibm-kubernetes-service-billing","text":"------------------ UNDER CONSTRUCTION ------------------ In this section we will explain techniques which can be used to track the relative resource consumption of individual namespaces/projects, when each tenant is deployed to a shared OpenShift/IBM Kubernetes cluster. We will describe how to achieve this on a per namespace/project level with IBM Cloud Monitoring . As all tenants share the same OpenShift/IBM Kubernetes cluster, standard IBM Cloud billing can be used to see this fixed cost. Each tenant will have their own service instances for Postgres and AppId, so we will show how standard IBM Cloud billing can be used to see costs per service instance (or resource group, if we decide to organise the services in this way). If however the business prefers to use Postgres in a multi-tenant way, we will also describe how to use IBM Cloud Monitoring to monitor Postgres 'per shema'.","title":"OpenShift/IBM Kubernetes Service Billing"},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/k8s-onboarding/","text":"OpenShift/IBM Kubernetes Service Onboarding \u00b6 ------------------ UNDER CONSTRUCTION ------------------ In this section we describe how to use the DevSecOps toolchains to onboard a new client. This will require adding a new tenant configuration file, using our scripts to provision a new instance of Postgres and AppId with service API keys, and triggering the CD pipeline after setting the environmental properties for the new tenant.","title":"Onboarding"},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/k8s-onboarding/#openshiftibm-kubernetes-service-onboarding","text":"------------------ UNDER CONSTRUCTION ------------------ In this section we describe how to use the DevSecOps toolchains to onboard a new client. This will require adding a new tenant configuration file, using our scripts to provision a new instance of Postgres and AppId with service API keys, and triggering the CD pipeline after setting the environmental properties for the new tenant.","title":"OpenShift/IBM Kubernetes Service Onboarding"},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/observability/","text":"Observability (logging, monitoring, vulnerabilities) \u00b6 ------------------ UNDER CONSTRUCTION ------------------ Visibility of OpenShift and Kubernetes clusters on IBM Cloud can easily be enhanced by utilizing existing IBM Cloud services. In this section, we will show: How IBM Log Analysis can be used to manage system and application logs. We will demonstrate how the sample application's logs can be filtered or searched, identifying logs on a per tenant basis for example. How IBM Cloud Monitoring can be used to gain operational visibility into the performance and health of the applications (e.g. the k8s deployments for each tenant), services (e.g. Postgres instances), and platforms (e.g. the health of the entire OpenShift or IBM Kubernetes Service cluster) How our sample DevSecOps toolchain uses DevOps Insights which can collect and analyze the results from unit tests, functional tests, and code coverage tools (using a variety of different sources). This provides visibility of quality, as policies at specified gates in the deployment process can be set, which can halt deployments preventing risks from being released. How our sample DevSecOps toolchain uses SonarCube to help ensure quality source code, how it can be configured, and how to resolve or whitelist issues. Explain how this relates or differs from Code Risk Analyzer . How our sample DevSecOps toolchain uses Continuous Delivery Vulnerability Advisor to help avoid container vulnerabilities, and how to resolve or whitelist the issues we encountered with our sample application.","title":"Observability (logging, monitoring, vulnerabilities)"},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/observability/#observability-logging-monitoring-vulnerabilities","text":"------------------ UNDER CONSTRUCTION ------------------ Visibility of OpenShift and Kubernetes clusters on IBM Cloud can easily be enhanced by utilizing existing IBM Cloud services. In this section, we will show: How IBM Log Analysis can be used to manage system and application logs. We will demonstrate how the sample application's logs can be filtered or searched, identifying logs on a per tenant basis for example. How IBM Cloud Monitoring can be used to gain operational visibility into the performance and health of the applications (e.g. the k8s deployments for each tenant), services (e.g. Postgres instances), and platforms (e.g. the health of the entire OpenShift or IBM Kubernetes Service cluster) How our sample DevSecOps toolchain uses DevOps Insights which can collect and analyze the results from unit tests, functional tests, and code coverage tools (using a variety of different sources). This provides visibility of quality, as policies at specified gates in the deployment process can be set, which can halt deployments preventing risks from being released. How our sample DevSecOps toolchain uses SonarCube to help ensure quality source code, how it can be configured, and how to resolve or whitelist issues. Explain how this relates or differs from Code Risk Analyzer . How our sample DevSecOps toolchain uses Continuous Delivery Vulnerability Advisor to help avoid container vulnerabilities, and how to resolve or whitelist the issues we encountered with our sample application.","title":"Observability (logging, monitoring, vulnerabilities)"},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/security-and-compliance/","text":"Observability (logging, monitoring, vulnerabilities) \u00b6 ------------------ UNDER CONSTRUCTION ------------------ Provide a brief summary of how the IBM Security and Compliance Center is embedded into our DevSecOps toolchains, and how it can be used to achieve a continuously secure and compliant development environment. We will discuss collecting audit information for regulated workloads, and using automation to avoid catastrophic mistakes. This topic only applies to toolchains created from the DevSecOps templates.","title":"Overview"},{"location":"kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/security-and-compliance/#observability-logging-monitoring-vulnerabilities","text":"------------------ UNDER CONSTRUCTION ------------------ Provide a brief summary of how the IBM Security and Compliance Center is embedded into our DevSecOps toolchains, and how it can be used to achieve a continuously secure and compliant development environment. We will discuss collecting audit information for regulated workloads, and using automation to avoid catastrophic mistakes. This topic only applies to toolchains created from the DevSecOps templates.","title":"Observability (logging, monitoring, vulnerabilities)"},{"location":"serverless-via-ibm-code-engine/ce-arcitecture/","text":"Architecture \u00b6 The easiest way to get started is to set up the sample application for two tenants on the IBM Cloud using serverless technology. The following diagram describes the serverless architecture of the simple e-commerce application which has two images (backend and frontend). Isolated Compute: One frontend container per tenant One backend container per tenant One App ID instance per tenant One Postgres instance (with one database) per tenant Shared CI/CD: One code base for frontend and backend services One image for frontend service One image for backend service One toolchain for all tenants (with four pipelines) Used IBM Services: IBM Code Engine IBM Container Registry IBM App ID IBM Postgres IBM Toolchain Used Technologies: Quarkus Vue.js and nginx Bash scripts","title":"Architecture"},{"location":"serverless-via-ibm-code-engine/ce-arcitecture/#architecture","text":"The easiest way to get started is to set up the sample application for two tenants on the IBM Cloud using serverless technology. The following diagram describes the serverless architecture of the simple e-commerce application which has two images (backend and frontend). Isolated Compute: One frontend container per tenant One backend container per tenant One App ID instance per tenant One Postgres instance (with one database) per tenant Shared CI/CD: One code base for frontend and backend services One image for frontend service One image for backend service One toolchain for all tenants (with four pipelines) Used IBM Services: IBM Code Engine IBM Container Registry IBM App ID IBM Postgres IBM Toolchain Used Technologies: Quarkus Vue.js and nginx Bash scripts","title":"Architecture"},{"location":"serverless-via-ibm-code-engine/ce-setup-create-the-instances/","text":"Getting started with the example application \u00b6 Create the tenants with the related IBM Cloud services The getting setup is only for the serverless part with Code Engine ! The objective is to provide you a getting started to basic understanding of the achitecture of the sample application with running instances. The image shows simplified what we are doing during the Getting Started . Here is an additional overview of tasks we automated for you with the getting started related to the diagram above. Create Code Engine projects Configure secret to access the IBM Cloud Container Registry Create secrets for the applications Configure the environment variables used by the applications App ID configurations PostgresSQL database Backend endpoint Build the container images and push them to the IBM Cloud Container Registry Create a PostgresSQL database service Setup example data for each tenant Create an App ID service Configure the service Prerequisites \u00b6 To follow the next steps you need to verify following two prerequisites. OS: macOS Container tool: podman If you don't use macOS you maybe need to install the listed cli tools according to your operating system. Only macOS in combination with podman and the SaaS Tools Image was verified by us. SaaS-Tools image \u00b6 For the simplification of the getting started we provide you a SaaS-Tools container image that contains all needed commandline tools for the automation with the bash scripts. Note: The bash automation is tailored to the bash scripting in Ubuntu, that is used as base OS for the SaaS-Tools image. Step 1: Open a terminal and start the SaaS-Tools image \u00b6 podman run -it --rm --privileged --name saas-tools \"quay.io/tsuedbroecker/saas-tools:v1\" Example output: root@7f535f968abc:/# Using the SaaS-Tools image \u00b6 From now we will only work inside the running SaaS-Tools container image. Step 1: Open the home directory \u00b6 cd home Step 2: Clone the repositories into the home directory \u00b6 git clone https://github.com/IBM/multi-tenancy git clone https://github.com/IBM/multi-tenancy-backend git clone https://github.com/IBM/multi-tenancy-frontend && cd multi-tenancy ROOT_FOLDER = $( pwd ) Step 3: Verify the prerequisites for running the installation \u00b6 cd $ROOT_FOLDER /installapp bash ./ce-check-prerequisites.sh The script stops when it notices any prerequisite is missing. Example output: Check prereqisites ... Success! All prerequisites verified! These are the tools we installed for you in the SaaS-Tools image: ibmcloud cli ibmcloud plugin code-engine ibmcloud plugin cloud-databases ibmcloud plugin container-registry buildah sed kubectl jq grep libpq (psql) cURL AWK Define the needed configurations for the tenants \u00b6 During the getting started we will setup two tenants of the sample application. Therefor we need to configure two kinds of configuration files. Global configuration Define the global configuration in global.json . It includes IBM Cloud settings such as region and resource group, container registry information and image information . Tenant-specific configuration For each tenant define tenant-specific configuration in the folder 'configuration/tenants'. That configuration contains for example App ID information . Postgres database information , application instance information , and Code Engine information . Here you find an example configuration tenant-a.json . Step 1: Configure IBM Cloud Container Registry Namespace in the global configuration \u00b6 The values for the names for a IBM Cloud Container Registry Namespace must unique in IBM Cloud for a region! To avoid problems during running the setup, please configure that name to your needs. Don't change one of the other default values, if you do not known what you are going to change. Open the globle.json nano ../configuration/global.json Replace the value for the namespace with a value of your choose: \"NAMESPACE\":\"multi-tenancy-example to \"NAMESPACE\":\"YOUR_VALUE\" . { \"IBM_CLOUD\" : { \"RESOURCE_GROUP\" : \"default\" , \"REGION\" : \"eu-de\" }, \"REGISTRY\" : { \"URL\" : \"de.icr.io\" , \"NAMESPACE\" : \"multi-tenancy-example\" , #< - INSERT YOUR VALUE \"TAG\" : \"v2\" , \"SECRET_NAME\" : \"multi.tenancy.cr.sec\" }, \"IMAGES\" : { \"NAME_BACKEND\" : \"multi-tenancy-service-backend\" , \"NAME_FRONTEND\" : \"multi-tenancy-service-frontend\" } } Step 2: Configure the Code Engine project name for each tenant \u00b6 The values for the names for an IBM Cloud Code Engine project must unique in IBM Cloud for a region! To avoid problems during running the setup, please configure these names to your needs. Don't change one of the other default values, if you do not known what you are going to change. Configure your Code Engine project names for the two tenants In the tenant-a.json files you can to change the value for the Code Engine project to something like multi-tenancy-example-mypostfix . Open the first tenant configuration tenant-a.json . (repeat these steps for the tenant-b.json file) nano ../configuration/tenants/tenant-a.json Replace the value for the project name of the Code Engine project to one of your choice: \"PROJECT_NAME\":\"multi-tenancy-serverless-a to \"PROJECT_NAME\":\"YOUR_VALUE\" . { \"APP_ID\" : { \"SERVICE_INSTANCE\" : \"multi-tenancy-serverless-appid-a\" , \"SERVICE_KEY_NAME\" : \"multi-tenancy-serverless-appid-key-a\" }, \"POSTGRES\" : { \"SERVICE_INSTANCE\" : \"multi-tenancy-serverless-pg-ten-a\" , \"SERVICE_KEY_NAME\" : \"multi-tenancy-serverless-pg-ten-a-key\" , \"SQL_FILE\" : \"create-populate-tenant-a.sql\" }, \"APPLICATION\" : { \"CONTAINER_NAME_BACKEND\" : \"multi-tenancy-service-backend-movies\" , \"CONTAINER_NAME_FRONTEND\" : \"multi-tenancy-service-frontend-movies\" , \"CATEGORY\" : \"Movies\" }, \"CODE_ENGINE\" : { \"PROJECT_NAME\" : \"multi-tenancy-serverless-a-t\" < - INSERT YOUR VALUE }, \"IBM_KUBERNETES_SERVICE\" : { \"NAME\" : \"niklas-heidloff3-fra04-b3c.4x16\" , \"NAMESPACE\" : \"tenant-a\" }, \"IBM_OPENSHIFT_SERVICE\" : { \"NAME\" : \"roks-gen2-suedbro\" , \"NAMESPACE\" : \"tenant-a\" }, \"PLATFORM\" : { \"NAME\" : \"IBM_OPENSHIFT_SERVICE\" } } Optional Step 3: Using a shared IBM Cloud Account \u00b6 The initial design of getting started setup has the assumption that you use an own IBM Cloud Account and not a shared one. In case if you use a shared IBM Cloud Account with other developers you need to ensure the AppID service and Postgres service configurations are unique for each tenant in the shared account. You need to change the specifications for the used services \"APP_ID\" and \"POSTGRES\" ! In that case you need to open up the tenants configurations again. Open the first tenant configuration tenant-a.json . (repeat these steps for the tenant-b.json file) nano ../configuration/tenants/tenant-a.json Replace the values for App ID and Postgres service your own choice: APP_ID.SERVICE_INSTANCE APP_ID.SERVICE_KEY_NAME POSTGRES.SERVICE_INSTANCE SERVICE_INSTANCE.SERVICE_KEY_NAME { \"APP_ID\" : { \"SERVICE_INSTANCE\" : \"multi-tenancy-serverless-appid-a\" , < - INSERT YOUR VALUE \"SERVICE_KEY_NAME\" : \"multi-tenancy-serverless-appid-key-a\" < - INSERT YOUR VALUE }, \"POSTGRES\" : { \"SERVICE_INSTANCE\" : \"multi-tenancy-serverless-pg-ten-a\" , < - INSERT YOUR VALUE \"SERVICE_KEY_NAME\" : \"multi-tenancy-serverless-pg-ten-a-key\" , < - INSERT YOUR VALUE \"SQL_FILE\" : \"create-populate-tenant-a.sql\" }, \"APPLICATION\" : { \"CONTAINER_NAME_BACKEND\" : \"multi-tenancy-service-backend-movies\" , \"CONTAINER_NAME_FRONTEND\" : \"multi-tenancy-service-frontend-movies\" , \"CATEGORY\" : \"Movies\" }, \"CODE_ENGINE\" : { \"PROJECT_NAME\" : \"multi-tenancy-serverless-a-t\" }, \"IBM_KUBERNETES_SERVICE\" : { \"NAME\" : \"niklas-heidloff3-fra04-b3c.4x16\" , \"NAMESPACE\" : \"tenant-a\" }, \"IBM_OPENSHIFT_SERVICE\" : { \"NAME\" : \"roks-gen2-suedbro\" , \"NAMESPACE\" : \"tenant-a\" }, \"PLATFORM\" : { \"NAME\" : \"IBM_OPENSHIFT_SERVICE\" } } Start the bash script automation \u00b6 Step 1: Open the folder for the getting started installation \u00b6 cd $ROOT_FOLDER /installapp Step 2: Log on with you IBM Cloud account \u00b6 ibmcloud login --sso Step 3: Start the bash automation with following command \u00b6 bash ./ce-create-two-tenancies.sh The execution takes roughly 30 minutes . The review steps during bash automation \u00b6 You will be asked to review some configurations and press enter to move forward in some steps. The script will stop in some situations when it discovers a problem during the setup. These are the situations where the script asks for review your configuration. a. It asks for review for the basic global configurations you made \u00b6 IBMCLOUD_CR_REGION_URL: de.icr.io IBMCLOUD_CR_NAMESPACE: multi-tenancy-example-t RESOURCE_GROUP : default REGION : eu-de ------------------------------ Verify the given entries and press return b. After the creation and upload of the container images to IBM Cloud registry it asks for the review of your first tenant configuration \u00b6 Parameter count : ../configuration/global.json ../configuration/tenants/tenant-a.json Parameter zero 'name of the script' : ./ce-install-application.sh --------------------------------- Global configuration : ../configuration/global.json Tenant configuration : ../configuration/tenants/tenant-a.json --------------------------------- Code Engine project : multi-tenancy-serverless-a-t --------------------------------- App ID service instance name : multi-tenancy-serverless-appid-a App ID service key name : multi-tenancy-serverless-appid-key-a --------------------------------- Application Service Catalog name : multi-tenancy-service-backend-movies Application Frontend name : multi-tenancy-service-frontend-movies Application Frontend category : Movies Application Service Catalog image: de.icr.io/multi-tenancy-example-t/multi-tenancy-service-backend:v2 Application Frontend image : de.icr.io/multi-tenancy-example-t/multi-tenancy-service-frontend:v2 --------------------------------- Postgres instance name : multi-tenancy-serverless-pg-ten-a Postgres service key name : multi-tenancy-serverless-pg-ten-a-key Postgres sample data sql : create-populate-tenant-a.sql --------------------------------- IBM Cloud Container Registry URL : de.icr.io --------------------------------- IBM Cloud RESOURCE_GROUP : default IBM Cloud REGION : eu-de --------------------------------- ------------------------------ Verify parameters and press return c. After each tenant creation the URL of the frontend applications will be displayed. \u00b6 ************************************ URLs ************************************ - oAuthServerUrl : https://eu-de.appid.cloud.ibm.com/oauth/v4/c74c03f2-03e9-4a4a-88cf-afc82573bae7 - discoveryEndpoint: https://eu-de.appid.cloud.ibm.com/oauth/v4/c74c03f2-03e9-4a4a-88cf-afc82573bae7/.well-known/openid-configuration - Frontend : https://multi-tenancy-service-frontend-movies.l1vbi06y6y4.eu-de.codeengine.appdomain.cloud - ServiceCatalog : https://multi-tenancy-service-backend-movies.l1vbi06y6y4.eu-de.codeengine.appdomain.cloud ------------------------------ Verify the given entries and press return ------------------------------ How to move on? \u00b6 You can move on understand the automation or skip that part and continue with Verify the created instances . Details to understand the automation \u00b6 Here are details related to the used bash scripts. We use three bash scripts for the initial installation. The following diagram shows the simplified dependencies of these bash scripts used to create two tenants of the example application on IBM Cloud in Code Engine. The scripts creating two tenants: Two Code Engine projects with two applications one frontend and one backend. Two App ID instance to provide a basic authentication and authorization for the two tenants. Two Postgres databases for the two tenants. The table contains the script and the responsibility of the scripts. Script Responsibility ce-create-two-tenancies.sh Build the container images for the frontend and backend, therefor it invokes the bash script ce-build-images-ibm-docker.sh and uploads the images to the IBM Cloud container registry. It also starts the creation of the tenant application instance, therefor it invokes the bash script ce-install-application.sh twice. ce-build-images-ibm-docker.sh Creates two container images based on the given parameters for the backend and frontend image names. ce-install-application.sh Creates and configures a Code Engine project . The configuration of the Code Engine project includes the creation of the application , the IBM Cloud Container Registry access therefor it also creates a IBM Cloud API and it creates the secrets for the needed parameter for the running applications. It creates an IBM Cloud App ID instance and configures this instance that includes the application , redirects , login layout , scope , role and user . It also creates an IBM Cloud Postgres database instance and creates the needed example data with tables inside the database. Know issues \u00b6 1. net/http: TLS handshake timeout \u00b6 During the creation and upload of the container image with buildah maybe you will notice like following. Put \"https://de.icr.io/v2/multi-tenancy-example-t/multi-tenancy-service-backend/blobs/uploads/25c9a689-8f60-43f3-b82f-a2fe91123ec4?_state=WMMtQe4a1npnpMoP90BkMkq0wkVPS7M1jcdYqkUqqul7Ik5hbWUiOiJtdWx0aS10ZW5hbmN5LWV4YW1wbGUtdC9tdWx0aS10ZW5hbmN5LXNlcnZpY2UtYmFja2VuZCIsIlVVSUQiOiIyNWM5YTY4OS04ZjYwLTQzZjMtYjgyZi1hMmZlOTExMjNlYzQiLCJPZmZzZXQiOjQxNzk4Njc0LCJTdGFydGVkQXQiOiIyMDIyLTAzLTAyVDE0OjExOjI1WiJ9&digest=sha256%3A8e3e6f15c16fce3aafe16ec9528311245ae74c7ce015ffaf9c796806ba02c8f8\" : net/http: TLS handshake timeout Solution It this case you can just start the script execution again. 2. Problem during the creation of resources \u00b6 If not all resource were created properly. Solution The easiest way to fix it is just to clean-up the IBM Cloud resources and restart the creation with the bash script. Here is the link to the Clean-Up section of the documentation for the serverless part. 3. Clean-Up IBM Cloud Code Engine Project name and IBM Cloud Container Registry Namespace name \u00b6 ATTENTION: The IBM Cloud Code Engine Project Name and the IBM Cloud Container Registry Namespace Name are unique inside an IBM Cloud region. If you delete a Container Registry namespace or a Code Engine project they can't just be recreated with the same name for a specific timeframe, they are now in a trash to give you a chance to restore them. If you want to restore them with the same name you need to follow the steps in the IBM Cloud documentation: IBM Cloud Container Registry - Cleaning up your namespaces IBM Cloud Code Engine ibmcloud ce reclamation restore The Clean-Up scripts don't using hard deletion!","title":"a. Create the tenants with the related IBM Cloud services"},{"location":"serverless-via-ibm-code-engine/ce-setup-create-the-instances/#getting-started-with-the-example-application","text":"Create the tenants with the related IBM Cloud services The getting setup is only for the serverless part with Code Engine ! The objective is to provide you a getting started to basic understanding of the achitecture of the sample application with running instances. The image shows simplified what we are doing during the Getting Started . Here is an additional overview of tasks we automated for you with the getting started related to the diagram above. Create Code Engine projects Configure secret to access the IBM Cloud Container Registry Create secrets for the applications Configure the environment variables used by the applications App ID configurations PostgresSQL database Backend endpoint Build the container images and push them to the IBM Cloud Container Registry Create a PostgresSQL database service Setup example data for each tenant Create an App ID service Configure the service","title":"Getting started with the example application"},{"location":"serverless-via-ibm-code-engine/ce-setup-create-the-instances/#prerequisites","text":"To follow the next steps you need to verify following two prerequisites. OS: macOS Container tool: podman If you don't use macOS you maybe need to install the listed cli tools according to your operating system. Only macOS in combination with podman and the SaaS Tools Image was verified by us.","title":"Prerequisites"},{"location":"serverless-via-ibm-code-engine/ce-setup-create-the-instances/#saas-tools-image","text":"For the simplification of the getting started we provide you a SaaS-Tools container image that contains all needed commandline tools for the automation with the bash scripts. Note: The bash automation is tailored to the bash scripting in Ubuntu, that is used as base OS for the SaaS-Tools image.","title":"SaaS-Tools image"},{"location":"serverless-via-ibm-code-engine/ce-setup-create-the-instances/#step-1-open-a-terminal-and-start-the-saas-tools-image","text":"podman run -it --rm --privileged --name saas-tools \"quay.io/tsuedbroecker/saas-tools:v1\" Example output: root@7f535f968abc:/#","title":"Step 1: Open a terminal and start the SaaS-Tools image"},{"location":"serverless-via-ibm-code-engine/ce-setup-create-the-instances/#using-the-saas-tools-image","text":"From now we will only work inside the running SaaS-Tools container image.","title":"Using the SaaS-Tools image"},{"location":"serverless-via-ibm-code-engine/ce-setup-create-the-instances/#step-1-open-the-home-directory","text":"cd home","title":"Step 1: Open the home directory"},{"location":"serverless-via-ibm-code-engine/ce-setup-create-the-instances/#step-2-clone-the-repositories-into-the-home-directory","text":"git clone https://github.com/IBM/multi-tenancy git clone https://github.com/IBM/multi-tenancy-backend git clone https://github.com/IBM/multi-tenancy-frontend && cd multi-tenancy ROOT_FOLDER = $( pwd )","title":"Step 2: Clone the repositories into the home directory"},{"location":"serverless-via-ibm-code-engine/ce-setup-create-the-instances/#step-3-verify-the-prerequisites-for-running-the-installation","text":"cd $ROOT_FOLDER /installapp bash ./ce-check-prerequisites.sh The script stops when it notices any prerequisite is missing. Example output: Check prereqisites ... Success! All prerequisites verified! These are the tools we installed for you in the SaaS-Tools image: ibmcloud cli ibmcloud plugin code-engine ibmcloud plugin cloud-databases ibmcloud plugin container-registry buildah sed kubectl jq grep libpq (psql) cURL AWK","title":"Step 3: Verify the prerequisites for running the installation"},{"location":"serverless-via-ibm-code-engine/ce-setup-create-the-instances/#define-the-needed-configurations-for-the-tenants","text":"During the getting started we will setup two tenants of the sample application. Therefor we need to configure two kinds of configuration files. Global configuration Define the global configuration in global.json . It includes IBM Cloud settings such as region and resource group, container registry information and image information . Tenant-specific configuration For each tenant define tenant-specific configuration in the folder 'configuration/tenants'. That configuration contains for example App ID information . Postgres database information , application instance information , and Code Engine information . Here you find an example configuration tenant-a.json .","title":"Define the needed configurations for the tenants"},{"location":"serverless-via-ibm-code-engine/ce-setup-create-the-instances/#step-1-configure-ibm-cloud-container-registry-namespace-in-the-global-configuration","text":"The values for the names for a IBM Cloud Container Registry Namespace must unique in IBM Cloud for a region! To avoid problems during running the setup, please configure that name to your needs. Don't change one of the other default values, if you do not known what you are going to change. Open the globle.json nano ../configuration/global.json Replace the value for the namespace with a value of your choose: \"NAMESPACE\":\"multi-tenancy-example to \"NAMESPACE\":\"YOUR_VALUE\" . { \"IBM_CLOUD\" : { \"RESOURCE_GROUP\" : \"default\" , \"REGION\" : \"eu-de\" }, \"REGISTRY\" : { \"URL\" : \"de.icr.io\" , \"NAMESPACE\" : \"multi-tenancy-example\" , #< - INSERT YOUR VALUE \"TAG\" : \"v2\" , \"SECRET_NAME\" : \"multi.tenancy.cr.sec\" }, \"IMAGES\" : { \"NAME_BACKEND\" : \"multi-tenancy-service-backend\" , \"NAME_FRONTEND\" : \"multi-tenancy-service-frontend\" } }","title":"Step 1: Configure IBM Cloud Container Registry Namespace in the global configuration"},{"location":"serverless-via-ibm-code-engine/ce-setup-create-the-instances/#step-2-configure-the-code-engine-project-name-for-each-tenant","text":"The values for the names for an IBM Cloud Code Engine project must unique in IBM Cloud for a region! To avoid problems during running the setup, please configure these names to your needs. Don't change one of the other default values, if you do not known what you are going to change. Configure your Code Engine project names for the two tenants In the tenant-a.json files you can to change the value for the Code Engine project to something like multi-tenancy-example-mypostfix . Open the first tenant configuration tenant-a.json . (repeat these steps for the tenant-b.json file) nano ../configuration/tenants/tenant-a.json Replace the value for the project name of the Code Engine project to one of your choice: \"PROJECT_NAME\":\"multi-tenancy-serverless-a to \"PROJECT_NAME\":\"YOUR_VALUE\" . { \"APP_ID\" : { \"SERVICE_INSTANCE\" : \"multi-tenancy-serverless-appid-a\" , \"SERVICE_KEY_NAME\" : \"multi-tenancy-serverless-appid-key-a\" }, \"POSTGRES\" : { \"SERVICE_INSTANCE\" : \"multi-tenancy-serverless-pg-ten-a\" , \"SERVICE_KEY_NAME\" : \"multi-tenancy-serverless-pg-ten-a-key\" , \"SQL_FILE\" : \"create-populate-tenant-a.sql\" }, \"APPLICATION\" : { \"CONTAINER_NAME_BACKEND\" : \"multi-tenancy-service-backend-movies\" , \"CONTAINER_NAME_FRONTEND\" : \"multi-tenancy-service-frontend-movies\" , \"CATEGORY\" : \"Movies\" }, \"CODE_ENGINE\" : { \"PROJECT_NAME\" : \"multi-tenancy-serverless-a-t\" < - INSERT YOUR VALUE }, \"IBM_KUBERNETES_SERVICE\" : { \"NAME\" : \"niklas-heidloff3-fra04-b3c.4x16\" , \"NAMESPACE\" : \"tenant-a\" }, \"IBM_OPENSHIFT_SERVICE\" : { \"NAME\" : \"roks-gen2-suedbro\" , \"NAMESPACE\" : \"tenant-a\" }, \"PLATFORM\" : { \"NAME\" : \"IBM_OPENSHIFT_SERVICE\" } }","title":"Step 2: Configure  the Code Engine project name for each tenant"},{"location":"serverless-via-ibm-code-engine/ce-setup-create-the-instances/#optional-step-3-using-a-shared-ibm-cloud-account","text":"The initial design of getting started setup has the assumption that you use an own IBM Cloud Account and not a shared one. In case if you use a shared IBM Cloud Account with other developers you need to ensure the AppID service and Postgres service configurations are unique for each tenant in the shared account. You need to change the specifications for the used services \"APP_ID\" and \"POSTGRES\" ! In that case you need to open up the tenants configurations again. Open the first tenant configuration tenant-a.json . (repeat these steps for the tenant-b.json file) nano ../configuration/tenants/tenant-a.json Replace the values for App ID and Postgres service your own choice: APP_ID.SERVICE_INSTANCE APP_ID.SERVICE_KEY_NAME POSTGRES.SERVICE_INSTANCE SERVICE_INSTANCE.SERVICE_KEY_NAME { \"APP_ID\" : { \"SERVICE_INSTANCE\" : \"multi-tenancy-serverless-appid-a\" , < - INSERT YOUR VALUE \"SERVICE_KEY_NAME\" : \"multi-tenancy-serverless-appid-key-a\" < - INSERT YOUR VALUE }, \"POSTGRES\" : { \"SERVICE_INSTANCE\" : \"multi-tenancy-serverless-pg-ten-a\" , < - INSERT YOUR VALUE \"SERVICE_KEY_NAME\" : \"multi-tenancy-serverless-pg-ten-a-key\" , < - INSERT YOUR VALUE \"SQL_FILE\" : \"create-populate-tenant-a.sql\" }, \"APPLICATION\" : { \"CONTAINER_NAME_BACKEND\" : \"multi-tenancy-service-backend-movies\" , \"CONTAINER_NAME_FRONTEND\" : \"multi-tenancy-service-frontend-movies\" , \"CATEGORY\" : \"Movies\" }, \"CODE_ENGINE\" : { \"PROJECT_NAME\" : \"multi-tenancy-serverless-a-t\" }, \"IBM_KUBERNETES_SERVICE\" : { \"NAME\" : \"niklas-heidloff3-fra04-b3c.4x16\" , \"NAMESPACE\" : \"tenant-a\" }, \"IBM_OPENSHIFT_SERVICE\" : { \"NAME\" : \"roks-gen2-suedbro\" , \"NAMESPACE\" : \"tenant-a\" }, \"PLATFORM\" : { \"NAME\" : \"IBM_OPENSHIFT_SERVICE\" } }","title":"Optional Step 3: Using a shared IBM Cloud Account"},{"location":"serverless-via-ibm-code-engine/ce-setup-create-the-instances/#start-the-bash-script-automation","text":"","title":"Start the bash script automation"},{"location":"serverless-via-ibm-code-engine/ce-setup-create-the-instances/#step-1-open-the-folder-for-the-getting-started-installation","text":"cd $ROOT_FOLDER /installapp","title":"Step 1: Open the folder for the getting started installation"},{"location":"serverless-via-ibm-code-engine/ce-setup-create-the-instances/#step-2-log-on-with-you-ibm-cloud-account","text":"ibmcloud login --sso","title":"Step 2: Log on with you IBM Cloud account"},{"location":"serverless-via-ibm-code-engine/ce-setup-create-the-instances/#step-3-start-the-bash-automation-with-following-command","text":"bash ./ce-create-two-tenancies.sh The execution takes roughly 30 minutes .","title":"Step 3: Start the bash automation with following command"},{"location":"serverless-via-ibm-code-engine/ce-setup-create-the-instances/#the-review-steps-during-bash-automation","text":"You will be asked to review some configurations and press enter to move forward in some steps. The script will stop in some situations when it discovers a problem during the setup. These are the situations where the script asks for review your configuration.","title":"The review steps during bash automation"},{"location":"serverless-via-ibm-code-engine/ce-setup-create-the-instances/#a-it-asks-for-review-for-the-basic-global-configurations-you-made","text":"IBMCLOUD_CR_REGION_URL: de.icr.io IBMCLOUD_CR_NAMESPACE: multi-tenancy-example-t RESOURCE_GROUP : default REGION : eu-de ------------------------------ Verify the given entries and press return","title":"a. It asks for review for the basic global configurations you made"},{"location":"serverless-via-ibm-code-engine/ce-setup-create-the-instances/#b-after-the-creation-and-upload-of-the-container-images-to-ibm-cloud-registry-it-asks-for-the-review-of-your-first-tenant-configuration","text":"Parameter count : ../configuration/global.json ../configuration/tenants/tenant-a.json Parameter zero 'name of the script' : ./ce-install-application.sh --------------------------------- Global configuration : ../configuration/global.json Tenant configuration : ../configuration/tenants/tenant-a.json --------------------------------- Code Engine project : multi-tenancy-serverless-a-t --------------------------------- App ID service instance name : multi-tenancy-serverless-appid-a App ID service key name : multi-tenancy-serverless-appid-key-a --------------------------------- Application Service Catalog name : multi-tenancy-service-backend-movies Application Frontend name : multi-tenancy-service-frontend-movies Application Frontend category : Movies Application Service Catalog image: de.icr.io/multi-tenancy-example-t/multi-tenancy-service-backend:v2 Application Frontend image : de.icr.io/multi-tenancy-example-t/multi-tenancy-service-frontend:v2 --------------------------------- Postgres instance name : multi-tenancy-serverless-pg-ten-a Postgres service key name : multi-tenancy-serverless-pg-ten-a-key Postgres sample data sql : create-populate-tenant-a.sql --------------------------------- IBM Cloud Container Registry URL : de.icr.io --------------------------------- IBM Cloud RESOURCE_GROUP : default IBM Cloud REGION : eu-de --------------------------------- ------------------------------ Verify parameters and press return","title":"b. After the creation and upload of the container images to IBM Cloud registry it asks for the review of your first tenant configuration"},{"location":"serverless-via-ibm-code-engine/ce-setup-create-the-instances/#c-after-each-tenant-creation-the-url-of-the-frontend-applications-will-be-displayed","text":"************************************ URLs ************************************ - oAuthServerUrl : https://eu-de.appid.cloud.ibm.com/oauth/v4/c74c03f2-03e9-4a4a-88cf-afc82573bae7 - discoveryEndpoint: https://eu-de.appid.cloud.ibm.com/oauth/v4/c74c03f2-03e9-4a4a-88cf-afc82573bae7/.well-known/openid-configuration - Frontend : https://multi-tenancy-service-frontend-movies.l1vbi06y6y4.eu-de.codeengine.appdomain.cloud - ServiceCatalog : https://multi-tenancy-service-backend-movies.l1vbi06y6y4.eu-de.codeengine.appdomain.cloud ------------------------------ Verify the given entries and press return ------------------------------","title":"c. After each tenant creation the URL of the frontend applications will be displayed."},{"location":"serverless-via-ibm-code-engine/ce-setup-create-the-instances/#how-to-move-on","text":"You can move on understand the automation or skip that part and continue with Verify the created instances .","title":"How to move on?"},{"location":"serverless-via-ibm-code-engine/ce-setup-create-the-instances/#details-to-understand-the-automation","text":"Here are details related to the used bash scripts. We use three bash scripts for the initial installation. The following diagram shows the simplified dependencies of these bash scripts used to create two tenants of the example application on IBM Cloud in Code Engine. The scripts creating two tenants: Two Code Engine projects with two applications one frontend and one backend. Two App ID instance to provide a basic authentication and authorization for the two tenants. Two Postgres databases for the two tenants. The table contains the script and the responsibility of the scripts. Script Responsibility ce-create-two-tenancies.sh Build the container images for the frontend and backend, therefor it invokes the bash script ce-build-images-ibm-docker.sh and uploads the images to the IBM Cloud container registry. It also starts the creation of the tenant application instance, therefor it invokes the bash script ce-install-application.sh twice. ce-build-images-ibm-docker.sh Creates two container images based on the given parameters for the backend and frontend image names. ce-install-application.sh Creates and configures a Code Engine project . The configuration of the Code Engine project includes the creation of the application , the IBM Cloud Container Registry access therefor it also creates a IBM Cloud API and it creates the secrets for the needed parameter for the running applications. It creates an IBM Cloud App ID instance and configures this instance that includes the application , redirects , login layout , scope , role and user . It also creates an IBM Cloud Postgres database instance and creates the needed example data with tables inside the database.","title":"Details to understand the automation"},{"location":"serverless-via-ibm-code-engine/ce-setup-create-the-instances/#know-issues","text":"","title":"Know issues"},{"location":"serverless-via-ibm-code-engine/ce-setup-create-the-instances/#1-nethttp-tls-handshake-timeout","text":"During the creation and upload of the container image with buildah maybe you will notice like following. Put \"https://de.icr.io/v2/multi-tenancy-example-t/multi-tenancy-service-backend/blobs/uploads/25c9a689-8f60-43f3-b82f-a2fe91123ec4?_state=WMMtQe4a1npnpMoP90BkMkq0wkVPS7M1jcdYqkUqqul7Ik5hbWUiOiJtdWx0aS10ZW5hbmN5LWV4YW1wbGUtdC9tdWx0aS10ZW5hbmN5LXNlcnZpY2UtYmFja2VuZCIsIlVVSUQiOiIyNWM5YTY4OS04ZjYwLTQzZjMtYjgyZi1hMmZlOTExMjNlYzQiLCJPZmZzZXQiOjQxNzk4Njc0LCJTdGFydGVkQXQiOiIyMDIyLTAzLTAyVDE0OjExOjI1WiJ9&digest=sha256%3A8e3e6f15c16fce3aafe16ec9528311245ae74c7ce015ffaf9c796806ba02c8f8\" : net/http: TLS handshake timeout Solution It this case you can just start the script execution again.","title":"1. net/http: TLS handshake timeout"},{"location":"serverless-via-ibm-code-engine/ce-setup-create-the-instances/#2-problem-during-the-creation-of-resources","text":"If not all resource were created properly. Solution The easiest way to fix it is just to clean-up the IBM Cloud resources and restart the creation with the bash script. Here is the link to the Clean-Up section of the documentation for the serverless part.","title":"2. Problem during the creation of resources"},{"location":"serverless-via-ibm-code-engine/ce-setup-create-the-instances/#3-clean-up-ibm-cloud-code-engine-project-name-and-ibm-cloud-container-registry-namespace-name","text":"ATTENTION: The IBM Cloud Code Engine Project Name and the IBM Cloud Container Registry Namespace Name are unique inside an IBM Cloud region. If you delete a Container Registry namespace or a Code Engine project they can't just be recreated with the same name for a specific timeframe, they are now in a trash to give you a chance to restore them. If you want to restore them with the same name you need to follow the steps in the IBM Cloud documentation: IBM Cloud Container Registry - Cleaning up your namespaces IBM Cloud Code Engine ibmcloud ce reclamation restore The Clean-Up scripts don't using hard deletion!","title":"3. Clean-Up IBM Cloud Code Engine Project name and IBM Cloud Container Registry Namespace name"},{"location":"serverless-via-ibm-code-engine/ce-verify-the-created-instances/","text":"Verify the created instances \u00b6 Verify the tenants with the related IBM Cloud services instances. Step 1: Verify the setup by using following url https://cloud.ibm.com/resources \u00b6 In resource list of the IBM Cloud UI, insert as filter for name the value multi . Now you should see following in your resource list: Step 2: Open App ID instance for tenant a and inspect the configuration \u00b6 Open following URL https://cloud.ibm.com/resources Step 3: Open Code Engine project for tenant a and inspect the project \u00b6 Open following link in a browser: https://cloud.ibm.com/codeengine/projects Step 4: Open the frontend application for tenant a in the Code Engine project \u00b6 Step 5: Click on URL and log on to the frontend application using username thomas@example.com and password thomas4appid \u00b6 Maybe you have to log on 5 times again, in case the Code Engine has scaled to zero before. How to move on? \u00b6 You can move with the CI/CD part or skip that part and continue with Clean-Up of the services and applications on IBM Cloud.","title":"b. Verify the tenants with the related IBM Cloud services instances"},{"location":"serverless-via-ibm-code-engine/ce-verify-the-created-instances/#verify-the-created-instances","text":"Verify the tenants with the related IBM Cloud services instances.","title":"Verify the created instances"},{"location":"serverless-via-ibm-code-engine/ce-verify-the-created-instances/#step-1-verify-the-setup-by-using-following-url-httpscloudibmcomresources","text":"In resource list of the IBM Cloud UI, insert as filter for name the value multi . Now you should see following in your resource list:","title":"Step 1: Verify the setup by using following url https://cloud.ibm.com/resources"},{"location":"serverless-via-ibm-code-engine/ce-verify-the-created-instances/#step-2-open-app-id-instance-for-tenant-a-and-inspect-the-configuration","text":"Open following URL https://cloud.ibm.com/resources","title":"Step 2: Open App ID instance for tenant a and inspect the configuration"},{"location":"serverless-via-ibm-code-engine/ce-verify-the-created-instances/#step-3-open-code-engine-project-for-tenant-a-and-inspect-the-project","text":"Open following link in a browser: https://cloud.ibm.com/codeengine/projects","title":"Step 3: Open Code Engine project for tenant a and inspect the project"},{"location":"serverless-via-ibm-code-engine/ce-verify-the-created-instances/#step-4-open-the-frontend-application-for-tenant-a-in-the-code-engine-project","text":"","title":"Step 4: Open the frontend application for tenant a in the Code Engine project"},{"location":"serverless-via-ibm-code-engine/ce-verify-the-created-instances/#step-5-click-on-url-and-log-on-to-the-frontend-application-using-username-thomasexamplecom-and-password-thomas4appid","text":"Maybe you have to log on 5 times again, in case the Code Engine has scaled to zero before.","title":"Step 5: Click on URL and log on to the frontend application using username thomas@example.com and password thomas4appid"},{"location":"serverless-via-ibm-code-engine/ce-verify-the-created-instances/#how-to-move-on","text":"You can move with the CI/CD part or skip that part and continue with Clean-Up of the services and applications on IBM Cloud.","title":"How to move on?"},{"location":"serverless-via-ibm-code-engine/ce_clean_up/","text":"Clean up \u00b6 In this section you will clean up all IBM Cloud resources we created during the getting started . ATTENTION: The IBM Cloud Code Engine Project Name and the IBM Cloud Container Registry Namespace Name are unique inside an IBM Cloud region. If you delete a Container Registry namespace or a Code Engine project they can't just be recreated with the same name for a specific timeframe, they are now in a trash to give you a chance to restore them. If you want to restore them with the same name you need to follow the steps in the IBM Cloud documentation: IBM Cloud Container Registry - Cleaning up your namespaces IBM Cloud Code Engine ibmcloud ce reclamation restore The Clean-Up scripts don't using hard deletion! Avoid project and namespace deletion \u00b6 Open the ce-clean-up.sh file cd $ROOT_FOLDER /installapp nano ce-clean-up.sh Comment out following steps cleanIBMContainerNamespace and cleanCodeEngineProject in the bash script # To avoid the deletion of the Container Registry Namespace # please comment out the `cleanIBMContainerNamespace` cleanIBMContainerNamespace ... # To avoid the deletion of the Code Engine project # please comment out the `cleanCodeEngineProject` cleanCodeEngineProject Clean-up \u00b6 The clean-up reuses the configuration json files you customized in the getting started section. To delete all created resources you execute following commands in the running SaaS Container Image : Step 1: Ensure you are in the installapp folder: \u00b6 cd $ROOT_FOLDER /installapp Step 2: Ensure you are logged on to IBM Cloud : \u00b6 ibmcloud login --sso Step 3: Start the deletion with the execution of the ce-clean-up-two-tenancies.sh script: \u00b6 sh ./ce-clean-up-two-tenancies.sh The review steps during bash automation \u00b6 The deletion will also ask to review some configurations and press enter to move forward in some steps. a. Verify the tenant-a details the script is going to delete \u00b6 Note: The same will happen for tenant-b . ************************************ Clean Tenant A ************************************ ************************************ Display parameter ************************************ count : ../configuration/global.json ../configuration/tenants/tenant-a.json Parameter zero 'name of the script' : ./ce-clean-up.sh --------------------------------- Global configuration : ../configuration/global.json Tenant configuration : ../configuration/tenants/tenant-a.json --------------------------------- Code Engine project : multi-tenancy-serverless-a --------------------------------- App ID service instance name : multi-tenancy-serverless-appid-a App ID service key name : multi-tenancy-serverless-appid-key-a --------------------------------- Application Service Catalog name : multi-tenancy-service-backend-movies Application Frontend name : multi-tenancy-service-frontend-movies Application Frontend category : Movies Application Service Catalog image: de.icr.io/multi-tenancy-example/multi-tenancy-service-backend:v2 Application Frontend image : de.icr.io/multi-tenancy-example/multi-tenancy-service-frontend:v2 --------------------------------- Postgres instance name : multi-tenancy-serverless-pg-ten-a Postgres service key name : multi-tenancy-serverless-pg-ten-a-key Postgres sample data sql : create-populate-tenant-a.sql --------------------------------- IBM Cloud Container Registry URL : de.icr.io Registry Namespace : multi-tenancy-example --------------------------------- IBM Cloud RESOURCE_GROUP : default IBM Cloud REGION : eu-de --------------------------------- Verify parameters and press return Details to understand the automation \u00b6 The table contains the scripts and the responsibilities of a script. Script Responsibility ce-clean-up-two-tenancies.sh It starts the clean-up for the tenant application instances, therefor it invokes the bash script ce-clean-up.sh twice with the json configuration files as parameters. ce-clean-up.sh Deletes all created resouce for the two tenants.","title":"Clean Up"},{"location":"serverless-via-ibm-code-engine/ce_clean_up/#clean-up","text":"In this section you will clean up all IBM Cloud resources we created during the getting started . ATTENTION: The IBM Cloud Code Engine Project Name and the IBM Cloud Container Registry Namespace Name are unique inside an IBM Cloud region. If you delete a Container Registry namespace or a Code Engine project they can't just be recreated with the same name for a specific timeframe, they are now in a trash to give you a chance to restore them. If you want to restore them with the same name you need to follow the steps in the IBM Cloud documentation: IBM Cloud Container Registry - Cleaning up your namespaces IBM Cloud Code Engine ibmcloud ce reclamation restore The Clean-Up scripts don't using hard deletion!","title":"Clean up"},{"location":"serverless-via-ibm-code-engine/ce_clean_up/#avoid-project-and-namespace-deletion","text":"Open the ce-clean-up.sh file cd $ROOT_FOLDER /installapp nano ce-clean-up.sh Comment out following steps cleanIBMContainerNamespace and cleanCodeEngineProject in the bash script # To avoid the deletion of the Container Registry Namespace # please comment out the `cleanIBMContainerNamespace` cleanIBMContainerNamespace ... # To avoid the deletion of the Code Engine project # please comment out the `cleanCodeEngineProject` cleanCodeEngineProject","title":"Avoid project and namespace deletion"},{"location":"serverless-via-ibm-code-engine/ce_clean_up/#clean-up_1","text":"The clean-up reuses the configuration json files you customized in the getting started section. To delete all created resources you execute following commands in the running SaaS Container Image :","title":"Clean-up"},{"location":"serverless-via-ibm-code-engine/ce_clean_up/#step-1-ensure-you-are-in-the-installapp-folder","text":"cd $ROOT_FOLDER /installapp","title":"Step 1: Ensure you are in the installapp folder:"},{"location":"serverless-via-ibm-code-engine/ce_clean_up/#step-2-ensure-you-are-logged-on-to-ibm-cloud","text":"ibmcloud login --sso","title":"Step 2: Ensure you are logged on to IBM Cloud:"},{"location":"serverless-via-ibm-code-engine/ce_clean_up/#step-3-start-the-deletion-with-the-execution-of-the-ce-clean-up-two-tenanciessh-script","text":"sh ./ce-clean-up-two-tenancies.sh","title":"Step 3: Start the deletion with the execution of the ce-clean-up-two-tenancies.sh script:"},{"location":"serverless-via-ibm-code-engine/ce_clean_up/#the-review-steps-during-bash-automation","text":"The deletion will also ask to review some configurations and press enter to move forward in some steps.","title":"The review steps during bash automation"},{"location":"serverless-via-ibm-code-engine/ce_clean_up/#a-verify-the-tenant-a-details-the-script-is-going-to-delete","text":"Note: The same will happen for tenant-b . ************************************ Clean Tenant A ************************************ ************************************ Display parameter ************************************ count : ../configuration/global.json ../configuration/tenants/tenant-a.json Parameter zero 'name of the script' : ./ce-clean-up.sh --------------------------------- Global configuration : ../configuration/global.json Tenant configuration : ../configuration/tenants/tenant-a.json --------------------------------- Code Engine project : multi-tenancy-serverless-a --------------------------------- App ID service instance name : multi-tenancy-serverless-appid-a App ID service key name : multi-tenancy-serverless-appid-key-a --------------------------------- Application Service Catalog name : multi-tenancy-service-backend-movies Application Frontend name : multi-tenancy-service-frontend-movies Application Frontend category : Movies Application Service Catalog image: de.icr.io/multi-tenancy-example/multi-tenancy-service-backend:v2 Application Frontend image : de.icr.io/multi-tenancy-example/multi-tenancy-service-frontend:v2 --------------------------------- Postgres instance name : multi-tenancy-serverless-pg-ten-a Postgres service key name : multi-tenancy-serverless-pg-ten-a-key Postgres sample data sql : create-populate-tenant-a.sql --------------------------------- IBM Cloud Container Registry URL : de.icr.io Registry Namespace : multi-tenancy-example --------------------------------- IBM Cloud RESOURCE_GROUP : default IBM Cloud REGION : eu-de --------------------------------- Verify parameters and press return","title":"a. Verify the tenant-a details the script is going to delete"},{"location":"serverless-via-ibm-code-engine/ce_clean_up/#details-to-understand-the-automation","text":"The table contains the scripts and the responsibilities of a script. Script Responsibility ce-clean-up-two-tenancies.sh It starts the clean-up for the tenant application instances, therefor it invokes the bash script ce-clean-up.sh twice with the json configuration files as parameters. ce-clean-up.sh Deletes all created resouce for the two tenants.","title":"Details to understand the automation"},{"location":"serverless-via-ibm-code-engine/code-engine-billing/","text":"Code Engine Billing \u00b6 ------------------ UNDER CONSTRUCTION ------------------ In this section we will explain the simplicity of billing when using Code Engine . As each tenant will be deployed as an individual Code Engine application, we will demonstrate how standard IBM Cloud tools can be used to determine per-tenant costs. Each tenant will have their own service instances for Postgres and AppId, so we will show how standard IBM Cloud billing can be used to see costs per service instance (or resource group, if we decide to organise the services in this way).","title":"Billing"},{"location":"serverless-via-ibm-code-engine/code-engine-billing/#code-engine-billing","text":"------------------ UNDER CONSTRUCTION ------------------ In this section we will explain the simplicity of billing when using Code Engine . As each tenant will be deployed as an individual Code Engine application, we will demonstrate how standard IBM Cloud tools can be used to determine per-tenant costs. Each tenant will have their own service instances for Postgres and AppId, so we will show how standard IBM Cloud billing can be used to see costs per service instance (or resource group, if we decide to organise the services in this way).","title":"Code Engine Billing"},{"location":"serverless-via-ibm-code-engine/code-engine-onboarding/","text":"Code Engine Onboarding \u00b6 ------------------ UNDER CONSTRUCTION ------------------ In this section we describe how to use the Code Engine DevOps toolchains to onboard a new client. This will require adding a new tenant configuration file, using our scripts to provision a new instance of Postgres and AppId with service API keys, and triggering the CD pipeline after setting the environmental properties for the new tenant.","title":"Onboarding"},{"location":"serverless-via-ibm-code-engine/code-engine-onboarding/#code-engine-onboarding","text":"------------------ UNDER CONSTRUCTION ------------------ In this section we describe how to use the Code Engine DevOps toolchains to onboard a new client. This will require adding a new tenant configuration file, using our scripts to provision a new instance of Postgres and AppId with service API keys, and triggering the CD pipeline after setting the environmental properties for the new tenant.","title":"Code Engine Onboarding"},{"location":"serverless-via-ibm-code-engine/observability/","text":"Observability (logging, monitoring, vulnerabilities) \u00b6 ------------------ UNDER CONSTRUCTION ------------------ Visibility of OpenShift and Kubernetes clusters on IBM Cloud can easily be enhanced by utilizing existing IBM Cloud services. In this section, we will show: How IBM Log Analysis can be used to manage system and application logs. We will demonstrate how the sample application's logs can be filtered or searched, identifying logs on a per tenant basis for example. How IBM Cloud Monitoring can be used to gain operational visibility into the performance and health of the applications (e.g. the k8s deployments for each tenant), services (e.g. Postgres instances), and platforms (e.g. the health of the entire OpenShift or IBM Kubernetes Service cluster) How our sample DevSecOps toolchain uses DevOps Insights which can collect and analyze the results from unit tests, functional tests, and code coverage tools (using a variety of different sources). This provides visibility of quality, as policies at specified gates in the deployment process can be set, which can halt deployments preventing risks from being released. How our sample DevSecOps toolchain uses SonarCube to help ensure quality source code, how it can be configured, and how to resolve or whitelist issues. Explain how this relates or differs from Code Risk Analyzer . How our sample DevSecOps toolchain uses Continuous Delivery Vulnerability Advisor to help avoid container vulnerabilities, and how to resolve or whitelist the issues we encountered with our sample application.","title":"Observability"},{"location":"serverless-via-ibm-code-engine/observability/#observability-logging-monitoring-vulnerabilities","text":"------------------ UNDER CONSTRUCTION ------------------ Visibility of OpenShift and Kubernetes clusters on IBM Cloud can easily be enhanced by utilizing existing IBM Cloud services. In this section, we will show: How IBM Log Analysis can be used to manage system and application logs. We will demonstrate how the sample application's logs can be filtered or searched, identifying logs on a per tenant basis for example. How IBM Cloud Monitoring can be used to gain operational visibility into the performance and health of the applications (e.g. the k8s deployments for each tenant), services (e.g. Postgres instances), and platforms (e.g. the health of the entire OpenShift or IBM Kubernetes Service cluster) How our sample DevSecOps toolchain uses DevOps Insights which can collect and analyze the results from unit tests, functional tests, and code coverage tools (using a variety of different sources). This provides visibility of quality, as policies at specified gates in the deployment process can be set, which can halt deployments preventing risks from being released. How our sample DevSecOps toolchain uses SonarCube to help ensure quality source code, how it can be configured, and how to resolve or whitelist issues. Explain how this relates or differs from Code Risk Analyzer . How our sample DevSecOps toolchain uses Continuous Delivery Vulnerability Advisor to help avoid container vulnerabilities, and how to resolve or whitelist the issues we encountered with our sample application.","title":"Observability (logging, monitoring, vulnerabilities)"},{"location":"serverless-via-ibm-code-engine/serverless-cicd/","text":"Simple Pipelines to update Serverless Application \u00b6 In order to update the backend and frontend containers on Code Engine, simple CI/CD pipelines are provided. pipeline-backend: Builds the backend image and triggers the deployment pipelines for all tenants pipeline-backend-tenant: Deploys the backend container for one tenant pipeline-frontend: Builds the frontend image and triggers the deployment pipelines for all tenants pipeline-frontend-tenant: Deploys the frontend container for one tenant The pipelines will use the configuration from the configuration directory in which global and tenant specific settings need to be defined. When the IBM Toolchain with the four pipelines is created, the four github.com/IBM/multi-tenancy* repos are cloned to your GitLab user accounts on the IBM Cloud. The toolchain can be created simply by invoking this URL: https://cloud.ibm.com/devops/setup/deploy?repository=https://github.com/ibm/multi-tenancy-serverless-ci-cd Note that on the first page the region and the resource group need to be the same ones as defined in configuration/global.json . Leave all other default values. On the second page you only need to create an API key. Leave all other default values. After you've created the toolchain, change your configuration in the 'configuration' directory of your GitLab repo. Then you can invoke the first pipeline \"pipeline-backend\" manually. Once the image has been built, it will trigger the deployment pipelines. The \"pipeline-frontend\" pipeline will only work after the backend has been deployed since the frontend containers need to know the endpoints of the backend containers. Step by Step Instructions \u00b6 Step 1: Ensure that the two Postgres and AppID service instances have been created. \u00b6 Step 2: Clone or fork the three multi-tenancy* repos: \u00b6 https://github.com/IBM/multi-tenancy https://github.com/IBM/multi-tenancy-backend https://github.com/IBM/multi-tenancy-frontend Step 3: Configure your application in the following three files (replace IBM with your account): \u00b6 https://github.com/IBM/multi-tenancy/blob/main/configuration/global.json https://github.com/IBM/multi-tenancy/blob/main/configuration/tenants/tenant-a.json https://github.com/IBM/multi-tenancy/blob/main/configuration/tenants/tenant-b.json Step 4: The toolchain can be created simply by invoking this URL: https://cloud.ibm.com/devops/setup/deploy?repository=https://github.com/ibm/multi-tenancy-serverless-ci-cd \u00b6 Replace the links to the three repos with your repos. Leave all other defaults on the first page. Step 5: On the second page create an IBM Cloud API key. \u00b6 Step 6: Click 'create' to create the toolchain. \u00b6 As a result you'll see these repos and pipelines: Step 7: When triggered, the backend pipeline will execute these tasks: \u00b6 Read configuration from JSON files Build the backend image and push it to the IBM container registry For each tenant invoke the pipeline-backend-tenant pipeline Step 8: For each tenant containers are deployed to Code Engine: \u00b6 Read tenant specific configuration Deploy to Code Engine Step 9: The backend containers have been deployed to Code Engine. \u00b6 Step 10: The (unprotected) backend endpoint can now be invoked via '.../category/2/products'. \u00b6 Step 11: After the backend has been deployed, the frontend can be deployed. The frontend needs to know the endpoint of the backend. \u00b6 Repeat the steps above to build the frontend image. Step 12: After the frontend image has been built, the containers are deployed for different tenants. \u00b6 Step 13: Once deployed, the frontend can be launched. \u00b6 User: thomas@example.com, password: thomas4appid","title":"CI/CD"},{"location":"serverless-via-ibm-code-engine/serverless-cicd/#simple-pipelines-to-update-serverless-application","text":"In order to update the backend and frontend containers on Code Engine, simple CI/CD pipelines are provided. pipeline-backend: Builds the backend image and triggers the deployment pipelines for all tenants pipeline-backend-tenant: Deploys the backend container for one tenant pipeline-frontend: Builds the frontend image and triggers the deployment pipelines for all tenants pipeline-frontend-tenant: Deploys the frontend container for one tenant The pipelines will use the configuration from the configuration directory in which global and tenant specific settings need to be defined. When the IBM Toolchain with the four pipelines is created, the four github.com/IBM/multi-tenancy* repos are cloned to your GitLab user accounts on the IBM Cloud. The toolchain can be created simply by invoking this URL: https://cloud.ibm.com/devops/setup/deploy?repository=https://github.com/ibm/multi-tenancy-serverless-ci-cd Note that on the first page the region and the resource group need to be the same ones as defined in configuration/global.json . Leave all other default values. On the second page you only need to create an API key. Leave all other default values. After you've created the toolchain, change your configuration in the 'configuration' directory of your GitLab repo. Then you can invoke the first pipeline \"pipeline-backend\" manually. Once the image has been built, it will trigger the deployment pipelines. The \"pipeline-frontend\" pipeline will only work after the backend has been deployed since the frontend containers need to know the endpoints of the backend containers.","title":"Simple Pipelines to update Serverless Application"},{"location":"serverless-via-ibm-code-engine/serverless-cicd/#step-by-step-instructions","text":"","title":"Step by Step Instructions"},{"location":"serverless-via-ibm-code-engine/serverless-cicd/#step-1-ensure-that-the-two-postgres-and-appid-service-instances-have-been-created","text":"","title":"Step 1: Ensure that the two Postgres and AppID service instances have been created."},{"location":"serverless-via-ibm-code-engine/serverless-cicd/#step-2-clone-or-fork-the-three-multi-tenancy-repos","text":"https://github.com/IBM/multi-tenancy https://github.com/IBM/multi-tenancy-backend https://github.com/IBM/multi-tenancy-frontend","title":"Step 2: Clone or fork the three multi-tenancy* repos:"},{"location":"serverless-via-ibm-code-engine/serverless-cicd/#step-3-configure-your-application-in-the-following-three-files-replace-ibm-with-your-account","text":"https://github.com/IBM/multi-tenancy/blob/main/configuration/global.json https://github.com/IBM/multi-tenancy/blob/main/configuration/tenants/tenant-a.json https://github.com/IBM/multi-tenancy/blob/main/configuration/tenants/tenant-b.json","title":"Step 3: Configure your application in the following three files (replace IBM with your account):"},{"location":"serverless-via-ibm-code-engine/serverless-cicd/#step-4-the-toolchain-can-be-created-simply-by-invoking-this-url-httpscloudibmcomdevopssetupdeployrepositoryhttpsgithubcomibmmulti-tenancy-serverless-ci-cd","text":"Replace the links to the three repos with your repos. Leave all other defaults on the first page.","title":"Step 4: The toolchain can be created simply by invoking this URL: https://cloud.ibm.com/devops/setup/deploy?repository=https://github.com/ibm/multi-tenancy-serverless-ci-cd"},{"location":"serverless-via-ibm-code-engine/serverless-cicd/#step-5-on-the-second-page-create-an-ibm-cloud-api-key","text":"","title":"Step 5: On the second page create an IBM Cloud API key."},{"location":"serverless-via-ibm-code-engine/serverless-cicd/#step-6-click-create-to-create-the-toolchain","text":"As a result you'll see these repos and pipelines:","title":"Step 6: Click 'create' to create the toolchain."},{"location":"serverless-via-ibm-code-engine/serverless-cicd/#step-7-when-triggered-the-backend-pipeline-will-execute-these-tasks","text":"Read configuration from JSON files Build the backend image and push it to the IBM container registry For each tenant invoke the pipeline-backend-tenant pipeline","title":"Step 7: When triggered, the backend pipeline will execute these tasks:"},{"location":"serverless-via-ibm-code-engine/serverless-cicd/#step-8-for-each-tenant-containers-are-deployed-to-code-engine","text":"Read tenant specific configuration Deploy to Code Engine","title":"Step 8: For each tenant containers are deployed to Code Engine:"},{"location":"serverless-via-ibm-code-engine/serverless-cicd/#step-9-the-backend-containers-have-been-deployed-to-code-engine","text":"","title":"Step 9: The backend containers have been deployed to Code Engine."},{"location":"serverless-via-ibm-code-engine/serverless-cicd/#step-10-the-unprotected-backend-endpoint-can-now-be-invoked-via-category2products","text":"","title":"Step 10: The (unprotected) backend endpoint can now be invoked via '.../category/2/products'."},{"location":"serverless-via-ibm-code-engine/serverless-cicd/#step-11-after-the-backend-has-been-deployed-the-frontend-can-be-deployed-the-frontend-needs-to-know-the-endpoint-of-the-backend","text":"Repeat the steps above to build the frontend image.","title":"Step 11: After the backend has been deployed, the frontend can be deployed. The frontend needs to know the endpoint of the backend."},{"location":"serverless-via-ibm-code-engine/serverless-cicd/#step-12-after-the-frontend-image-has-been-built-the-containers-are-deployed-for-different-tenants","text":"","title":"Step 12: After the frontend image has been built, the containers are deployed for different tenants."},{"location":"serverless-via-ibm-code-engine/serverless-cicd/#step-13-once-deployed-the-frontend-can-be-launched","text":"User: thomas@example.com, password: thomas4appid","title":"Step 13: Once deployed, the frontend can be launched."}]}